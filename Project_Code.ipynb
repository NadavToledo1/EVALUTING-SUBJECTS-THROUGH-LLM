{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ym1cX1pnc-zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import faiss\n",
        "import requests\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score"
      ],
      "metadata": {
        "id": "G_oKaKqqbiiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "8htvYDX1b9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrkDygLpa_Ir"
      },
      "outputs": [],
      "source": [
        "ip_port = \"{ip}:{port}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL and payload\n",
        "url = f\"http://{ip_port}/api/pull\""
      ],
      "metadata": {
        "id": "i_V6QMGtbeLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"model\": \"deepseek-r1:32b\"\n",
        "}\n",
        "\n",
        "# Make a POST request\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "# Check the response\n",
        "if response.status_code == 200:\n",
        "    print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "ISuhUW-Rbhfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"http://{ip_port}/api/generate\""
      ],
      "metadata": {
        "id": "Nx89aufablJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the connection to the llm"
      ],
      "metadata": {
        "id": "2dHrVua_d0nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"model\": \"deepseek-r1:32b\",\n",
        "    \"prompt\": \"what is systematic review?\",\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "# Make a POST request\n",
        "response = requests.post(url, json=payload)\n",
        "response\n",
        "# Print the response\n",
        "if response.status_code == 200:\n",
        "    print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "CgvUQOL2boiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = \"\"\n",
        "\n",
        "for line in response.text.splitlines():\n",
        "    try:\n",
        "        data = json.loads(line)  # Parse each JSON line\n",
        "        if \"response\" in data:  # Check if the \"response\" field exists\n",
        "            full_text += data[\"response\"]  # Append the text\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Unable to parse line:\", line)\n",
        "full_text"
      ],
      "metadata": {
        "id": "HcOBi7gPbsxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for llm response"
      ],
      "metadata": {
        "id": "A31wmSLXd5KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class LLMResponse:\n",
        "    content: str\n",
        "    score: float = None\n",
        "    topic: str = None\n",
        "    confidence: float = None\n",
        "    error: str = None\n",
        "    metadata: Dict[str, Any] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.metadata is None:\n",
        "            self.metadata = {}"
      ],
      "metadata": {
        "id": "jVSH-IKVbyvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for document"
      ],
      "metadata": {
        "id": "0_VPoTn0d7oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Document:\n",
        "    content: str\n",
        "    embedding: np.ndarray = None\n",
        "    metadata: Dict[str, Any] = None"
      ],
      "metadata": {
        "id": "DE9WkF3LcAOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-augmented generation class"
      ],
      "metadata": {
        "id": "yM4aqoKad-FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self, embedding_model=\"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initializes the RAG system:\n",
        "        Loads the specified embedding model.\n",
        "        Creates an empty document store and sets the FAISS index to None.\n",
        "        \"\"\"\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.document_store = []\n",
        "        self.index = None\n",
        "\n",
        "    def add_documents(self, documents: List[str]):\n",
        "        \"\"\"\n",
        "        Adds documents to the system and prepares them for retrieval\n",
        "        Converts each document into a vector embedding using the SentenceTransformer model.\n",
        "        Each document is wrapped in a Document object containing:\n",
        "          content: The text of the document.\n",
        "          embedding: The corresponding embedding (as a NumPy array).\n",
        "        These objects are appended to document_store.\n",
        "          \"\"\"\n",
        "        embeddings = self.embedding_model.encode(documents, convert_to_tensor=True)\n",
        "        for doc, emb in zip(documents, embeddings):\n",
        "            self.document_store.append(Document(\n",
        "                content=doc,\n",
        "                embedding=emb.numpy()\n",
        "            ))\n",
        "        self._update_index()\n",
        "\n",
        "    def _update_index(self):\n",
        "        \"\"\"\n",
        "        Updates the FAISS index with embeddings from all stored documents\n",
        "        \"\"\"\n",
        "        embeddings = np.vstack([doc.embedding for doc in self.document_store])\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "    def retrieve_relevant_docs(self, query: str, k: int = 3):\n",
        "        \"\"\"\n",
        "        Retrieves the top-k documents most relevant to a given query\n",
        "        \"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "        D, I = self.index.search(query_embedding.reshape(1, -1).astype('float32'), k)\n",
        "        return [self.document_store[i] for i in I[0]]"
      ],
      "metadata": {
        "id": "CKBU_wY-cCYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to find certain pattern in text"
      ],
      "metadata": {
        "id": "0WXep3vbeG15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pattern_safely(pattern, text, default=None):\n",
        "    if not text:\n",
        "        return default\n",
        "    try:\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return default"
      ],
      "metadata": {
        "id": "jhHr_tTjcE9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to clean topic from text"
      ],
      "metadata": {
        "id": "1rfMLOtweTqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_topic(topic):\n",
        "    if not topic:\n",
        "        return None\n",
        "\n",
        "    topic = re.sub(r'^\\d+\\.\\s*', '', topic)\n",
        "    topic = re.sub(r'^-\\s*', '', topic)\n",
        "    topic = re.sub(r'^$', '', topic)\n",
        "    topic = re.sub(r'\\b\\d+millisecond\\b', '', topic)\n",
        "    topic = re.sub(r'\\s+and\\s+', ' & ', topic)\n",
        "    topic = ' '.join(topic.split())\n",
        "    topic = topic.strip()\n",
        "\n",
        "    if len(topic) < 3:\n",
        "        return None\n",
        "\n",
        "    return topic"
      ],
      "metadata": {
        "id": "fcZTxi0bcG_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to clean and preprocess text"
      ],
      "metadata": {
        "id": "dLQISPEcecwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    custom_stops = {'would', 'could', 'should', 'said', 'like', 'also'}\n",
        "    stop_words.update(custom_stops)\n",
        "\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if (token not in stop_words and\n",
        "            len(token) > 2 and\n",
        "            not token.isnumeric() and\n",
        "            not all(c in '0123456789.-' for c in token)):\n",
        "\n",
        "            if token.isupper() and len(token) <= 5:\n",
        "                filtered_tokens.append(token)\n",
        "            else:\n",
        "                filtered_tokens.append(token.lower())\n",
        "\n",
        "    cleaned_text = \" \".join(filtered_tokens)\n",
        "    return cleaned_text if cleaned_text.strip() else \"placeholder\""
      ],
      "metadata": {
        "id": "zolNpaSncI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to clean and preprocess group of documents"
      ],
      "metadata": {
        "id": "afuQPMFdgchp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_documents(documents):\n",
        "    processed_docs = []\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            if isinstance(doc, str):\n",
        "                # Basic cleaning\n",
        "                doc = doc.lower()\n",
        "                doc = re.sub(r'\\s+', ' ', doc)\n",
        "                doc = re.sub(r'[^\\w\\s-]', '', doc)\n",
        "\n",
        "                # Tokenize\n",
        "                tokens = word_tokenize(doc)\n",
        "                stop_words = set(stopwords.words('english'))\n",
        "                custom_stops = {'would', 'could', 'should', 'said', 'like', 'also'}\n",
        "                stop_words.update(custom_stops)\n",
        "\n",
        "                # Filter tokens\n",
        "                filtered_tokens = []\n",
        "                for token in tokens:\n",
        "                    if (token not in stop_words and\n",
        "                        len(token) > 2 and\n",
        "                        not token.isnumeric() and\n",
        "                        not all(c in '0123456789.-' for c in token)):\n",
        "                        filtered_tokens.append(token.lower())\n",
        "\n",
        "                if filtered_tokens:  # Only append if we have tokens\n",
        "                    processed_docs.append(filtered_tokens)\n",
        "                else:\n",
        "                    processed_docs.append(['placeholder'])  # Add placeholder if no tokens\n",
        "            else:\n",
        "                processed_docs.append(['placeholder'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preprocessing document: {e}\")\n",
        "            processed_docs.append(['placeholder'])  # Add placeholder on error\n",
        "\n",
        "    return processed_docs"
      ],
      "metadata": {
        "id": "DKZlLmzccLWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to perform lda analysis on documents and returns topics"
      ],
      "metadata": {
        "id": "nA1Gt96feliK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_lda_analysis(documents, n_topics=5):\n",
        "    if not documents or not isinstance(documents, list):\n",
        "        return {\"assigned_topics\": [], \"topics_keywords\": {}}\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer(\n",
        "            stop_words='english',\n",
        "            max_df=0.95,\n",
        "            min_df=2,\n",
        "            token_pattern=r'(?u)\\b\\w+\\b'\n",
        "        )\n",
        "\n",
        "        X = vectorizer.fit_transform(documents)\n",
        "\n",
        "        lda_model = LatentDirichletAllocation(\n",
        "            n_components=n_topics,\n",
        "            random_state=42,\n",
        "            max_iter=20,\n",
        "            learning_method='batch'\n",
        "        )\n",
        "\n",
        "        lda_model.fit(X)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        topics = {}\n",
        "        for topic_idx, topic in enumerate(lda_model.components_):\n",
        "            top_keywords_idx = topic.argsort()[-10:][::-1]\n",
        "            top_keywords = [feature_names[i] for i in top_keywords_idx]\n",
        "            topics[topic_idx] = top_keywords\n",
        "\n",
        "        topic_assignments = lda_model.transform(X)\n",
        "        assigned_topics = np.argmax(topic_assignments, axis=1)\n",
        "\n",
        "        return {\n",
        "            \"assigned_topics\": assigned_topics.tolist(),\n",
        "            \"topics_keywords\": topics\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LDA analysis: {str(e)}\")\n",
        "        return {\"assigned_topics\": [], \"topics_keywords\": {}}"
      ],
      "metadata": {
        "id": "eFqYTNSRcgtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function that calculate coherence scores on group of documents"
      ],
      "metadata": {
        "id": "4yS6UCvbewoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_coherence_scores(groups, dictionary, measure=\"c_v\"):\n",
        "    scores = []\n",
        "    for group in groups:\n",
        "        try:\n",
        "            # Create \"topics\" as a list of the most frequent terms in the group\n",
        "            topics = [[word for word, freq in dictionary.doc2bow(doc)] for doc in group]\n",
        "\n",
        "            # Create a CoherenceModel for the group\n",
        "            coherence_model = CoherenceModel(\n",
        "                topics=topics,\n",
        "                texts=group,\n",
        "                dictionary=dictionary,\n",
        "                coherence=measure\n",
        "            )\n",
        "\n",
        "            # Calculate the coherence score\n",
        "            score = coherence_model.get_coherence()\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating coherence for group: {e}\")\n",
        "            scores.append(0.0)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "FmXqY6KycZ01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedLLMProcessor:\n",
        "    def __init__(self, api_key: str, model: str = \"deepseek-r1:32b\"):\n",
        "\n",
        "        self.model = model\n",
        "        self.api_url = url\n",
        "        self.prompts = {\n",
        "            \"summarize\": \"\"\"You are a technical document analyzer specializing in extracting key information from texts.\n",
        "\n",
        "TEXT TO ANALYZE:\n",
        "{text}\n",
        "\n",
        "RELEVANT CONTEXT:\n",
        "{context}\n",
        "\n",
        "Provide your analysis in this EXACT format:\n",
        "MAIN_TOPIC: [primary technical/scientific field]\n",
        "KEY_TERMS: [list only the most relevant technical terms, comma-separated]\n",
        "SUMMARY: [2-3 concise, technical sentences capturing the essence]\"\"\",\n",
        "\n",
        "            \"topic\": \"\"\"You are an expert topic classifier focusing on technical and academic content.\n",
        "\n",
        "Document for classification:\n",
        "{text}\n",
        "\n",
        "Analyze this document following these steps:\n",
        "1. Identify primary technical domain\n",
        "2. Extract key technical terminology\n",
        "3. Recognize methodological approaches\n",
        "4. Note any cross-domain elements\n",
        "\n",
        "Provide classification in this EXACT format:\n",
        "PRIMARY_TOPIC: [single specific technical field]\n",
        "SUBTOPICS: [3-4 related technical areas]\n",
        "TECHNICAL_INDICATORS: [key technical terms that influenced classification]\n",
        "CROSS_DOMAIN_ELEMENTS: [any interdisciplinary aspects]\n",
        "CONFIDENCE: [0-1 score with brief justification]\"\"\",\n",
        "\n",
        "            \"grade\": \"\"\"You are a specialized content coherence evaluator.\n",
        "\n",
        "TARGET GROUP:\n",
        "{documents}\n",
        "\n",
        "COMPARISON GROUPS:\n",
        "{other_groups}\n",
        "\n",
        "Evaluation Criteria:\n",
        "1. INTERNAL COHERENCE (50%)\n",
        "- How consistently do the documents align in topic and terminology?\n",
        "- Do they share a common technical vocabulary?\n",
        "- Is there thematic continuity?\n",
        "\n",
        "2. EXTERNAL DISTINCTIVENESS (50%)\n",
        "- How clearly separated is this group from others?\n",
        "- Are there unique technical markers?\n",
        "- Is there minimal topic overlap with other groups?\n",
        "\n",
        "IMPORTANT SCORING GUIDELINES:\n",
        "- Groups that are inconsistent or contain off-topic entries should receive a COHERENCE_SCORE below 5.\n",
        "- If group topics overlap significantly with other groups, DISTINCTIVENESS_SCORE should be below 5.\n",
        "- Use the full range of 1–10.\n",
        "- Be strict in low-quality cases.\n",
        "\n",
        "Examples:\n",
        "  - Mixed documents with weak cohesion → COHERENCE_SCORE: 3\n",
        "  - Unclear group separation → DISTINCTIVENESS_SCORE: 4\n",
        "  - Strong thematic continuity → COHERENCE_SCORE: 9\"\"\"\n",
        "        }\n",
        "\n",
        "    def process_text(self, text: str, task: str, additional_context: Dict = None) -> LLMResponse:\n",
        "        try:\n",
        "            prompt_template = self.prompts.get(task)\n",
        "            if not prompt_template:\n",
        "                raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "            context = {\n",
        "                \"text\": text,\n",
        "                \"context\": \"\",\n",
        "                \"documents\": \"\",\n",
        "                \"other_groups\": \"\"\n",
        "            }\n",
        "            if additional_context:\n",
        "                context.update(additional_context)\n",
        "\n",
        "            prompt = prompt_template.format(**context)\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"prompt\": prompt,\n",
        "                \"temperature\": 0.2,\n",
        "                \"top_p\": 0.2,\n",
        "                \"stream\": False\n",
        "            }\n",
        "\n",
        "            response = requests.post(self.api_url, json=payload)\n",
        "            response.raise_for_status()\n",
        "            content = response.json()[\"response\"].strip()\n",
        "\n",
        "            if task == \"grade\":\n",
        "                score = self._extract_score_from_response(content)\n",
        "                return LLMResponse(content=content, score=score, metadata={})\n",
        "            elif task == \"topic\":\n",
        "                topic_info = self._extract_topic_info(content)\n",
        "                return LLMResponse(\n",
        "                    content=content,\n",
        "                    topic=topic_info[\"primary_topic\"],\n",
        "                    confidence=topic_info[\"confidence\"],\n",
        "                    metadata=topic_info\n",
        "                )\n",
        "\n",
        "            return LLMResponse(content=content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in processing {task}: {e}\")\n",
        "            return LLMResponse(content=\"\", error=str(e))\n",
        "\n",
        "    def _extract_score_from_response(self, response_text):\n",
        "        try:\n",
        "            final_match = re.search(r'FINAL_SCORE:\\s*(\\d+)', response_text)\n",
        "            if final_match:\n",
        "                return int(final_match.group(1))\n",
        "            return 5\n",
        "        except Exception:\n",
        "            return 5\n",
        "\n",
        "    def _extract_topic_info(self, response_text):\n",
        "        try:\n",
        "            return {\n",
        "                \"primary_topic\": self._find(r'PRIMARY_TOPIC:\\s*([^\\n]+)', response_text, \"unknown\"),\n",
        "                \"subtopics\": self._find(r'SUBTOPICS:\\s*([^\\n]+)', response_text, \"\").split(','),\n",
        "                \"technical_indicators\": self._find(r'TECHNICAL_INDICATORS:\\s*([^\\n]+)', response_text, \"\").split(','),\n",
        "                \"cross_domain\": self._find(r'CROSS_DOMAIN_ELEMENTS:\\s*([^\\n]+)', response_text, \"\").split(','),\n",
        "                \"confidence\": float(self._find(r'CONFIDENCE:\\s*(0\\.\\d+|1\\.0)', response_text, \"0.5\"))\n",
        "            }\n",
        "        except Exception:\n",
        "            return {\n",
        "                \"primary_topic\": \"unknown\",\n",
        "                \"subtopics\": [],\n",
        "                \"technical_indicators\": [],\n",
        "                \"cross_domain\": [],\n",
        "                \"confidence\": 0.5\n",
        "            }\n",
        "\n",
        "    def _find(self, pattern, text, default):\n",
        "        match = re.search(pattern, text)\n",
        "        return match.group(1).strip() if match else default\n",
        "\n",
        "    def _calculate_group_coherence(self, documents):\n",
        "        \"\"\"Calculate internal coherence of a group of documents\"\"\"\n",
        "        try:\n",
        "            processed_docs = preprocess_documents(documents)\n",
        "            dictionary = Dictionary(processed_docs)\n",
        "            coherence_scores = calculate_coherence_scores([processed_docs], dictionary)\n",
        "            return coherence_scores[0] if coherence_scores else 0.0\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating group coherence: {e}\")\n",
        "            return 0.0"
      ],
      "metadata": {
        "id": "ZoKHRBg3cOVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_processor = EnhancedLLMProcessor(url)"
      ],
      "metadata": {
        "id": "o4AemA_QgxY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function that assign topic to group of documents in case topic is not given"
      ],
      "metadata": {
        "id": "OK3zsRF7gzGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_topic_to_group(documents, n_topics=5):\n",
        "    # Get LDA topics\n",
        "    lda_results = perform_lda_analysis(documents, n_topics)\n",
        "\n",
        "    # Get LLM topic analysis\n",
        "    llm_topics = set()  # Use set for automatic deduplication\n",
        "    try:\n",
        "        for doc in documents:\n",
        "            response = llm_processor.process_text(doc, \"topic\")\n",
        "            if response and not response.error and response.content:\n",
        "                # Extract topics with fallbacks\n",
        "                primary = find_pattern_safely(r'PRIMARY_TOPIC:\\s*([^\\n]+)', response.content)\n",
        "                subtopics = find_pattern_safely(r'SUBTOPICS:\\s*([^\\n]+)', response.content)\n",
        "                tech_indicators = find_pattern_safely(r'TECHNICAL_INDICATORS:\\s*([^\\n]+)', response.content)\n",
        "                cross_domain = find_pattern_safely(r'CROSS_DOMAIN_ELEMENTS:\\s*([^\\n]+)', response.content)\n",
        "\n",
        "                # Process primary topic\n",
        "                if primary:\n",
        "                    clean_primary = clean_topic(primary)\n",
        "                    if clean_primary:\n",
        "                        llm_topics.add(clean_primary)\n",
        "\n",
        "                # Process subtopics\n",
        "                if subtopics:\n",
        "                    for topic in subtopics.split(','):\n",
        "                        clean_sub = clean_topic(topic)\n",
        "                        if clean_sub:\n",
        "                            llm_topics.add(clean_sub)\n",
        "\n",
        "                # Process technical indicators\n",
        "                if tech_indicators:\n",
        "                    for term in tech_indicators.split(','):\n",
        "                        clean_term = clean_topic(term)\n",
        "                        if clean_term and len(clean_term.split()) > 1:  # Only add multi-word technical terms\n",
        "                            llm_topics.add(clean_term)\n",
        "\n",
        "                # Process cross-domain elements\n",
        "                if cross_domain:\n",
        "                    for element in cross_domain.split(','):\n",
        "                        clean_element = clean_topic(element)\n",
        "                        if clean_element:\n",
        "                            llm_topics.add(clean_element)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in LLM topic analysis: {str(e)}\")\n",
        "\n",
        "    # Convert set to sorted list for consistent output\n",
        "    llm_topics_list = sorted(list(llm_topics))\n",
        "\n",
        "    # Group similar topics\n",
        "    grouped_topics = []\n",
        "    processed = set()\n",
        "\n",
        "    for topic in llm_topics_list:\n",
        "        if topic in processed:\n",
        "            continue\n",
        "\n",
        "        similar_topics = [topic]\n",
        "        processed.add(topic)\n",
        "\n",
        "        # Find similar topics\n",
        "        for other in llm_topics_list:\n",
        "            if other not in processed:\n",
        "                # Check if topics are very similar\n",
        "                if (topic.lower() in other.lower() or\n",
        "                    other.lower() in topic.lower() or\n",
        "                    len(set(topic.lower().split()) & set(other.lower().split())) >= 2):\n",
        "                    similar_topics.append(other)\n",
        "                    processed.add(other)\n",
        "\n",
        "        # Add the main topic or the shortest similar topic\n",
        "        if len(similar_topics) > 1:\n",
        "            grouped_topics.append(min(similar_topics, key=len))\n",
        "        else:\n",
        "            grouped_topics.append(topic)\n",
        "\n",
        "    result = {\n",
        "        \"lda_results\": lda_results,\n",
        "        \"llm_topics\": grouped_topics,\n",
        "        \"combined_analysis\": {\n",
        "            \"assigned_topics\": lda_results[\"assigned_topics\"],\n",
        "            \"topics_keywords\": lda_results[\"topics_keywords\"],\n",
        "            \"llm_suggested_topics\": grouped_topics\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "03FPiCfScbi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function that get a dataset, group categories and documents per category and return balanced dataset for the experiment"
      ],
      "metadata": {
        "id": "jcQk88_1hAte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_balanced_dataset(dataset, category_groups, docs_per_category=3):\n",
        "\n",
        "    group_docs = []\n",
        "    category_counts = {}\n",
        "\n",
        "    for group_categories in category_groups:\n",
        "        group_data = []\n",
        "        group_total = 0\n",
        "\n",
        "        for category in group_categories:\n",
        "            category_indices = [i for i in range(len(dataset.target))\n",
        "                              if dataset.target_names[dataset.target[i]] == category]\n",
        "\n",
        "            # Get and preprocess documents\n",
        "            category_docs = [preprocess_text(dataset.data[i])\n",
        "                           for i in category_indices[:docs_per_category]]\n",
        "            group_data.extend(category_docs)\n",
        "\n",
        "            category_counts[category] = len(category_docs)\n",
        "            group_total += len(category_docs)\n",
        "\n",
        "        group_docs.append(group_data)\n",
        "\n",
        "        print(f\"\\nGroup with categories {group_categories}:\")\n",
        "        print(f\"Total documents: {group_total}\")\n",
        "        for category in group_categories:\n",
        "            print(f\"  - {category}: {category_counts[category]} documents\")\n",
        "\n",
        "    return group_docs, category_counts"
      ],
      "metadata": {
        "id": "vkBn5rTQcslV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function that evaluate the topics and group documents several times for more consistent and reliable results"
      ],
      "metadata": {
        "id": "Tc_6HhzVhM3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multiple_times(group1, group2, group3, topics=None, num_iterations=3):\n",
        "\n",
        "    if topics is None:\n",
        "        topics = ['Technology', 'Scientific', 'Social/Political']\n",
        "\n",
        "    scores = {\n",
        "        'llm_scores': {topic: [] for topic in topics},\n",
        "        'coherence_scores': {topic: [] for topic in topics}\n",
        "    }\n",
        "\n",
        "    print(\"Summarizing documents...\")\n",
        "    summarized_group1 = [llm_processor.process_text(doc, \"summarize\").content for doc in group1]\n",
        "    summarized_group2 = [llm_processor.process_text(doc, \"summarize\").content for doc in group2]\n",
        "    summarized_group3 = [llm_processor.process_text(doc, \"summarize\").content for doc in group3]\n",
        "\n",
        "    # Preprocess all groups once\n",
        "    all_groups = [group1, group2, group3]\n",
        "    tokenized_groups = [preprocess_documents(group) for group in all_groups]\n",
        "\n",
        "    # Create dictionary from all documents\n",
        "    all_docs_tokenized = [token for group in tokenized_groups for token in group]\n",
        "    dictionary = Dictionary(all_docs_tokenized)\n",
        "    dictionary.filter_extremes(no_below=2, no_above=0.95)  # Filter extreme terms\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        print(f\"\\nIteration {i + 1}/{num_iterations}\")\n",
        "        print(\"Performing LLM evaluation...\")\n",
        "\n",
        "        for j, (group, topic) in enumerate(zip([summarized_group1, summarized_group2, summarized_group3], topics)):\n",
        "            other_groups = [g for k, g in enumerate([summarized_group1, summarized_group2, summarized_group3]) if k != j]\n",
        "\n",
        "            # Call LLM and store response\n",
        "            response = llm_processor.process_text(\"\", \"grade\", {\n",
        "                \"documents\": \"\\n\".join(group),\n",
        "                \"other_groups\": \"\\n\".join([\"\\n\".join(g) for g in other_groups])\n",
        "            })\n",
        "\n",
        "            # 🔍 Print raw LLM output for inspection\n",
        "            print(f\"\\n🔍 Raw LLM Grade Response for topic '{topic}':\\n{response.content}\\n\")\n",
        "\n",
        "            # Store score\n",
        "            scores['llm_scores'][topic].append(response.score)\n",
        "\n",
        "\n",
        "    print(\"Calculating coherence scores...\")\n",
        "    coherence_scores = calculate_coherence_scores(tokenized_groups, dictionary)\n",
        "    for topic, score in zip(topics, coherence_scores):\n",
        "        scores['coherence_scores'][topic] = [score]  # Single coherence score per group\n",
        "\n",
        "    # Calculate results\n",
        "    results = {}\n",
        "    for score_type in ['llm_scores', 'coherence_scores']:\n",
        "        results[score_type] = {\n",
        "            'scores': scores[score_type],\n",
        "            'avg': {\n",
        "                topic: sum(topic_scores) / len(topic_scores) if topic_scores else 0.0\n",
        "                for topic, topic_scores in scores[score_type].items()\n",
        "            },\n",
        "            'std': {\n",
        "                topic: (\n",
        "                    (sum((x - (sum(topic_scores) / len(topic_scores))) ** 2 for x in topic_scores) / len(topic_scores)) ** 0.5\n",
        "                    if len(topic_scores) > 0 else 0.0\n",
        "                )\n",
        "                for topic, topic_scores in scores[score_type].items()\n",
        "            }\n",
        "        }\n",
        "    return results"
      ],
      "metadata": {
        "id": "Go_8QQ5QcwAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
