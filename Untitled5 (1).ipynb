{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73162dec-0359-45c1-b8b9-3ae2eec68c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import faiss\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from transformers import pipeline\n",
    "\n",
    "# ======================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ======================================\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# ======================================\n",
    "# GOLDEN EXAMPLES FOR CALIBRATION\n",
    "# ======================================\n",
    "\n",
    "# Define golden examples of document coherence with perfect scores\n",
    "GOLDEN_EXAMPLES = [\n",
    "    {\n",
    "        \"group_name\": \"Golden Example: Technology\",\n",
    "        \"documents\": [\n",
    "            \"Cloud computing has transformed how businesses manage their IT infrastructure. Companies can now scale their computing resources on demand without investing in physical hardware. Services like AWS, Azure, and Google Cloud provide flexible options for storage, computation, and specialized services.\",\n",
    "            \"Edge computing is gaining popularity as IoT devices proliferate. By processing data closer to where it's generated rather than sending everything to centralized cloud servers, edge computing reduces latency and bandwidth usage. Smart cities and autonomous vehicles benefit greatly from this distributed computing approach.\",\n",
    "            \"Quantum computing promises to revolutionize computational capabilities for specific problems. Using quantum bits or qubits that can exist in multiple states simultaneously, these systems can potentially solve complex optimization problems exponentially faster than classical computers.\"\n",
    "        ],\n",
    "        \"coherence_score\": 10,\n",
    "        \"explanation\": \"These documents all discuss modern computing paradigms (cloud, edge, and quantum computing). They share technical vocabulary, focus on the same general domain of computing infrastructure, and each explains how a specific technology impacts computing capabilities. This group demonstrates perfect coherence with a clear unified theme.\"\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"Golden Example: Partially Related\",\n",
    "        \"documents\": [\n",
    "            \"Renewable energy sources like solar and wind power are becoming increasingly important in the global energy mix. As technology improves and costs decrease, these clean energy options are becoming more competitive with fossil fuels.\",\n",
    "            \"Electric vehicles are gaining market share in the automotive industry. Major manufacturers are investing billions in developing new EV models with longer ranges and shorter charging times to appeal to mainstream consumers.\",\n",
    "            \"Urban planning in modern cities increasingly incorporates green spaces and pedestrian-friendly zones. These design choices help reduce urban heat islands and improve air quality for residents.\"\n",
    "        ],\n",
    "        \"coherence_score\": 5,\n",
    "        \"explanation\": \"These documents share some thematic connections around sustainability and modern infrastructure, but discuss different specific topics (energy production, transportation, and urban design). They have partial topical overlap through environmental themes, but each focuses on a distinct domain with different terminology and concepts. This represents moderate coherence with some connecting threads but no single unified topic.\"\n",
    "    },\n",
    "    {\n",
    "        \"group_name\": \"Golden Example: Mixed Topics\",\n",
    "        \"documents\": [\n",
    "            \"Photosynthesis is the process by which plants convert light energy into chemical energy. This process produces oxygen as a byproduct and is essential for maintaining Earth's atmosphere.\",\n",
    "            \"The French Revolution began in 1789 and led to far-reaching social and political changes in France. Key events included the Storming of the Bastille and the Reign of Terror.\",\n",
    "            \"JavaScript is a programming language commonly used for web development. It allows developers to create interactive elements on websites and runs directly in the user's browser.\"\n",
    "        ],\n",
    "        \"coherence_score\": 1,\n",
    "        \"explanation\": \"These documents cover completely different topics (biology, history, and computer science) with no meaningful connection between them. They use different terminology, discuss unrelated concepts, and share no common themes. This group demonstrates minimum coherence with no unified topic.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ======================================\n",
    "# DATA CLASSES\n",
    "# ======================================\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Represents a response from a language model.\"\"\"\n",
    "    content: str\n",
    "    score: float = None\n",
    "    topic: str = None\n",
    "    confidence: float = None\n",
    "    error: str = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a document with its content and embedding.\"\"\"\n",
    "    content: str\n",
    "    embedding: np.ndarray = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "# ======================================\n",
    "# RAG SYSTEM\n",
    "# ======================================\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system for document retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the RAG system:\n",
    "        Loads the specified embedding model.\n",
    "        Creates an empty document store and sets the FAISS index to None.\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.document_store = []\n",
    "        self.index = None\n",
    "\n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Adds documents to the system and prepares them for retrieval.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get embeddings from the model\n",
    "            embeddings = self.embedding_model.encode(documents, convert_to_tensor=True)\n",
    "            \n",
    "            # Process each document and its embedding\n",
    "            for doc, emb in zip(documents, embeddings):\n",
    "                # Convert tensor to numpy, handling GPU tensors if needed\n",
    "                if hasattr(emb, 'is_cuda') and emb.is_cuda:\n",
    "                    # If tensor is on GPU, move to CPU first\n",
    "                    numpy_emb = emb.cpu().numpy()\n",
    "                else:\n",
    "                    # If already on CPU\n",
    "                    numpy_emb = emb.numpy()\n",
    "                \n",
    "                # Add document to store\n",
    "                self.document_store.append(Document(\n",
    "                    content=doc,\n",
    "                    embedding=numpy_emb\n",
    "                ))\n",
    "            \n",
    "            # Update the search index\n",
    "            self._update_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to RAG system: {e}\")\n",
    "            # Provide more detailed error info for debugging\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def _update_index(self):\n",
    "        \"\"\"\n",
    "        Updates the FAISS index with embeddings from all stored documents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.document_store:\n",
    "                print(\"Warning: Document store is empty, no index created\")\n",
    "                return\n",
    "                \n",
    "            # Stack all embeddings\n",
    "            embeddings = np.vstack([doc.embedding for doc in self.document_store])\n",
    "            dimension = embeddings.shape[1]\n",
    "            \n",
    "            # Create and populate the index\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating FAISS index: {e}\")\n",
    "\n",
    "    def retrieve_relevant_docs(self, query: str, k: int = 3):\n",
    "        \"\"\"\n",
    "        Retrieves the top-k documents most relevant to a given query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.index:\n",
    "                print(\"Warning: Index not initialized, no documents can be retrieved\")\n",
    "                return []\n",
    "                \n",
    "            # Encode the query\n",
    "            query_embedding = self.embedding_model.encode([query])[0]\n",
    "            \n",
    "            # Handle GPU tensor if needed\n",
    "            if hasattr(query_embedding, 'is_cuda') and query_embedding.is_cuda:\n",
    "                query_embedding = query_embedding.cpu()\n",
    "                \n",
    "            # Reshape and convert to the right format\n",
    "            query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "            \n",
    "            # Search the index\n",
    "            D, I = self.index.search(query_embedding, k)\n",
    "            \n",
    "            # Return the matching documents\n",
    "            return [self.document_store[i] for i in I[0] if i < len(self.document_store)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            return []\n",
    "\n",
    "# ======================================\n",
    "# TEXT PROCESSING UTILITIES\n",
    "# ======================================\n",
    "def find_pattern_safely(pattern, text, default=None):\n",
    "    \"\"\"\n",
    "    Safely extract content matching a regex pattern with robust error handling.\n",
    "    \n",
    "    This function searches for a specified regex pattern in the provided text and\n",
    "    returns the first matching group if found. It includes multiple safeguards:\n",
    "    - Handles None or empty text input\n",
    "    - Uses try-except to catch any regex-related errors\n",
    "    - Supports a default return value for failed matches\n",
    "    - Applies case-insensitive, multiline, and dot-all regex flags\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pattern : str\n",
    "        The regular expression pattern to search for. Should include at least one\n",
    "        capturing group, as the function returns the content of the first group.\n",
    "        \n",
    "    text : str or None\n",
    "        The text to search within. Can be None or empty string, in which case\n",
    "        the default value is returned.\n",
    "        \n",
    "    default : any, optional\n",
    "        The value to return if no match is found or an error occurs.\n",
    "        Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or default\n",
    "        The content of the first capturing group if a match is found,\n",
    "        or the default value if no match or an error occurs.\n",
    "        The returned string is stripped of leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return default\n",
    "    try:\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error matching pattern {pattern}: {e}\")\n",
    "    return default\n",
    "\n",
    "\n",
    "def clean_topic(topic):\n",
    "    \"\"\"\n",
    "    Clean and normalize a topic string by removing artifacts and standardizing format.\n",
    "    \n",
    "    This function performs the following transformations:\n",
    "    - Removes numeric prefixes (e.g., \"1. Topic\" -> \"Topic\")\n",
    "    - Removes dash prefixes (e.g., \"- Topic\" -> \"Topic\")\n",
    "    - Removes empty strings\n",
    "    - Removes specific terms like \"millisecond\"\n",
    "    - Replaces \" and \" with \" & \"\n",
    "    - Normalizes whitespace\n",
    "    - Trims leading/trailing whitespace\n",
    "    - Rejects very short topics (fewer than 3 characters)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic : str or None\n",
    "        The topic string to clean and normalize. Can be None, in which case\n",
    "        None is returned.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str or None\n",
    "        The cleaned and normalized topic string, or None if the input is None,\n",
    "        empty, or results in a topic shorter than 3 characters after cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not topic:\n",
    "        return None\n",
    "\n",
    "    topic = re.sub(r'^\\d+\\.\\s*', '', topic)\n",
    "    topic = re.sub(r'^-\\s*', '', topic)\n",
    "    topic = re.sub(r'^$', '', topic)\n",
    "    topic = re.sub(r'\\b\\d+millisecond\\b', '', topic)\n",
    "    topic = re.sub(r'\\s+and\\s+', ' & ', topic)\n",
    "    topic = ' '.join(topic.split())\n",
    "    topic = topic.strip()\n",
    "\n",
    "    if len(topic) < 3:\n",
    "        return None\n",
    "\n",
    "    return topic\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text content for analysis or processing.\n",
    "    \n",
    "    This function applies a sequence of cleaning operations to prepare text data:\n",
    "    - Handles non-string inputs by returning empty string\n",
    "    - Removes URLs and web links\n",
    "    - Removes HTML tags\n",
    "    - Removes special characters and punctuation\n",
    "    - Removes numbers and digits\n",
    "    - Normalizes whitespace (multiple spaces, tabs, newlines)\n",
    "    - Converts text to lowercase\n",
    "    - Trims leading/trailing whitespace\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str or any\n",
    "        The text to clean. If not a string, returns an empty string.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The cleaned and normalized text content as a lowercase string\n",
    "        with standardized spacing and without URLs, HTML, special characters,\n",
    "        or numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip and lower case\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess text by removing stopwords and keeping only alphabetic tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw text to be preprocessed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        List of tokenized words with stopwords removed\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in word_tokenize(text.lower()) if word.isalpha() and word not in stop_words]\n",
    "\n",
    "def preprocess_text_minimal(text):\n",
    "    \"\"\"\n",
    "    Perform minimal text preprocessing while preserving core document content and meaning.\n",
    "    \n",
    "    This function applies a lightweight preprocessing pipeline designed to normalize text\n",
    "    while maintaining the semantic content as much as possible. Unlike more aggressive\n",
    "    preprocessing approaches, this function focuses on essential normalization steps\n",
    "    with minimal information loss.\n",
    "    \n",
    "    The preprocessing steps include:\n",
    "    1. Handling empty or non-string inputs\n",
    "    2. Replacing URLs with a generic 'URL' token\n",
    "    3. Replacing numbers with a generic 'NUM' token\n",
    "    4. Removing special characters and punctuation\n",
    "    5. Normalizing whitespace (multiple spaces, tabs, newlines)\n",
    "    6. Converting to lowercase\n",
    "    7. Removing a minimal set of basic English stopwords\n",
    "    8. Filtering out tokens with 2 or fewer characters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : Any\n",
    "        The text to preprocess. If None, empty, or not a string, returns \"empty document\".\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The preprocessed text as a space-separated string of filtered tokens.\n",
    "        Returns \"empty document\" if the input is invalid or if no tokens remain\n",
    "        after filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"empty document\"\n",
    "    \n",
    "    # Very light cleaning\n",
    "    text = re.sub(r'http\\S+', 'URL', text)  # Replace URLs\n",
    "    text = re.sub(r'\\d+', 'NUM', text)  # Replace numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Keep alphanumeric only\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove only basic stopwords\n",
    "    basic_stops = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at'}\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [t for t in tokens if t not in basic_stops and len(t) > 2]\n",
    "    \n",
    "    return \" \".join(filtered_tokens) if filtered_tokens else \"empty document\"\n",
    "\n",
    "def preprocess_documents(documents, aggressive=False):\n",
    "    \"\"\"\n",
    "    Process a collection of documents with improved tokenization and error handling.\n",
    "    \n",
    "    This function applies preprocessing to each document in a collection, converting\n",
    "    them to tokenized format suitable for NLP tasks. It includes robust error handling\n",
    "    to ensure all documents are processed, even if individual documents cause errors.\n",
    "    \n",
    "    The processing pipeline:\n",
    "    1. Verifies each item is a string\n",
    "    2. Applies minimal text preprocessing (via preprocess_text_minimal)\n",
    "    3. Tokenizes the text using NLTK's word_tokenize\n",
    "    4. Handles empty token lists by inserting a placeholder\n",
    "    5. Handles non-string inputs and errors by using placeholders\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : List[str] or Iterable[Any]\n",
    "        A collection of documents to process. Ideally strings, but the function\n",
    "        handles non-string elements gracefully by replacing them with placeholders.\n",
    "        \n",
    "    aggressive : bool, default=False\n",
    "        Flag to control preprocessing intensity (currently not used but kept\n",
    "        for backward compatibility or future implementation of more aggressive\n",
    "        preprocessing options).\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[str]]\n",
    "        A list of tokenized documents, where each document is represented as\n",
    "        a list of tokens. Documents that couldn't be processed properly will\n",
    "        contain a single token: 'placeholder'.\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            if isinstance(doc, str):\n",
    "                # Use minimal preprocessing\n",
    "                cleaned_text = preprocess_text_minimal(doc)\n",
    "                tokens = word_tokenize(cleaned_text)\n",
    "                \n",
    "                if tokens:  # Only append if we have tokens\n",
    "                    processed_docs.append(tokens)\n",
    "                else:\n",
    "                    processed_docs.append(['placeholder'])\n",
    "            else:\n",
    "                processed_docs.append(['placeholder'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing document: {e}\")\n",
    "            processed_docs.append(['placeholder'])\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert NumPy objects to JSON-serializable Python types.\n",
    "    \n",
    "    This utility function converts NumPy types (integers, floats, arrays) to their\n",
    "    standard Python equivalents to enable JSON serialization. It's particularly useful\n",
    "    when preparing data for JSON output, API responses, or storing in document databases.\n",
    "    \n",
    "    The function handles the following NumPy types:\n",
    "    - np.integer → Python int\n",
    "    - np.floating → Python float\n",
    "    - np.ndarray → Python list\n",
    "    - Other types are returned unchanged\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    obj : Any\n",
    "        The object to convert. This can be a NumPy scalar (np.int64, np.float32, etc.),\n",
    "        a NumPy array, or any other object. Non-NumPy objects are returned as-is.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Any\n",
    "        The converted object that can be safely serialized to JSON. NumPy types are\n",
    "        converted to their Python equivalents, while other objects are returned unchanged.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "# ======================================\n",
    "# LLM PROMPTS\n",
    "# ======================================\n",
    "\n",
    "def get_improved_summarize_prompt():\n",
    "    \"\"\"Get prompt for document summarization.\"\"\"\n",
    "    return \"\"\"You are a document analyzer. Extract the key content accurately.\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Identify the ACTUAL topic (e.g., sports, religion, technology, politics)\n",
    "2. Extract specific details mentioned in the text\n",
    "3. Note any important context or background\n",
    "\n",
    "PROVIDE EXACTLY THIS FORMAT:\n",
    "MAIN_TOPIC: [what is this document actually about?]\n",
    "CATEGORY: [broad category: sports, religion, technology, politics, science, etc.]\n",
    "KEY_DETAILS: [specific facts, names, or events mentioned]\n",
    "SUMMARY: [1-2 sentences capturing the essence]\"\"\"\n",
    "\n",
    "def get_enhanced_grade_prompt():\n",
    "    \"\"\"Get prompt for coherence evaluation.\"\"\"\n",
    "    return \"\"\"You are a document coherence evaluator. Assess how similar these documents are to each other.\n",
    "\n",
    "CALIBRATION EXAMPLES:\n",
    "Example 1 - Perfect Coherence (Score: 10):\n",
    "Documents: [\n",
    "  \"Cloud computing has transformed how businesses manage their IT infrastructure. Companies can now scale their computing resources on demand without investing in physical hardware. Services like AWS, Azure, and Google Cloud provide flexible options for storage, computation, and specialized services.\",\n",
    "  \"Edge computing is gaining popularity as IoT devices proliferate. By processing data closer to where it's generated rather than sending everything to centralized cloud servers, edge computing reduces latency and bandwidth usage. Smart cities and autonomous vehicles benefit greatly from this distributed computing approach.\",\n",
    "  \"Quantum computing promises to revolutionize computational capabilities for specific problems. Using quantum bits or qubits that can exist in multiple states simultaneously, these systems can potentially solve complex optimization problems exponentially faster than classical computers.\"\n",
    "]\n",
    "Explanation: These documents all discuss modern computing paradigms (cloud, edge, and quantum computing). They share technical vocabulary, focus on the same general domain of computing infrastructure, and each explains how a specific technology impacts computing capabilities. This group demonstrates perfect coherence with a clear unified theme.\n",
    "\n",
    "Example 2 - Moderate Coherence (Score: 5):\n",
    "Documents: [\n",
    "  \"Renewable energy sources like solar and wind power are becoming increasingly important in the global energy mix. As technology improves and costs decrease, these clean energy options are becoming more competitive with fossil fuels.\",\n",
    "  \"Electric vehicles are gaining market share in the automotive industry. Major manufacturers are investing billions in developing new EV models with longer ranges and shorter charging times to appeal to mainstream consumers.\",\n",
    "  \"Urban planning in modern cities increasingly incorporates green spaces and pedestrian-friendly zones. These design choices help reduce urban heat islands and improve air quality for residents.\"\n",
    "]\n",
    "Explanation: These documents share some thematic connections around sustainability and modern infrastructure, but discuss different specific topics (energy production, transportation, and urban design). They have partial topical overlap through environmental themes, but each focuses on a distinct domain with different terminology and concepts. This represents moderate coherence with some connecting threads but no single unified topic.\n",
    "\n",
    "Example 3 - No Coherence (Score: 1):\n",
    "Documents: [\n",
    "  \"Photosynthesis is the process by which plants convert light energy into chemical energy. This process produces oxygen as a byproduct and is essential for maintaining Earth's atmosphere.\",\n",
    "  \"The French Revolution began in 1789 and led to far-reaching social and political changes in France. Key events included the Storming of the Bastille and the Reign of Terror.\",\n",
    "  \"JavaScript is a programming language commonly used for web development. It allows developers to create interactive elements on websites and runs directly in the user's browser.\"\n",
    "]\n",
    "Explanation: These documents cover completely different topics (biology, history, and computer science) with no meaningful connection between them. They use different terminology, discuss unrelated concepts, and share no common themes. This group demonstrates minimum coherence with no unified topic.\n",
    "\n",
    "TARGET GROUP TO ANALYZE:\n",
    "{documents}\n",
    "SCORING GUIDE - YOU MUST USE THE FULL RANGE:\n",
    "10: All documents are clearly about the SAME SPECIFIC topic (e.g., all about basketball strategies, all about AI applications)\n",
    "8-9: Documents about the same topic with minimal variations or subtopic differences\n",
    "6-7: Documents in the same general field but discussing different aspects/subtopics\n",
    "4-5: Documents with weak connections, mostly different topics with some overlap\n",
    "1-3: Documents on completely different topics with little to no meaningful connection\n",
    "\n",
    "EVALUATION INSTRUCTIONS:\n",
    "1. Identify the precise topic of each document\n",
    "2. If ALL documents focus on the SAME SPECIFIC TOPIC (e.g., all about basketball), you MUST score 9-10\n",
    "3. If documents share NO connection at all, you MUST score 1-2\n",
    "4. Use the middle scores (3-8) only when documents have partial overlap\n",
    "5. BE GENEROUS with high scores (9-10) when documents clearly share the same topic\n",
    "\n",
    "PROVIDE EXACTLY THIS FORMAT:\n",
    "COHERENCE_SCORE: [1-10]\n",
    "MAIN_TOPICS: [list actual topics found in documents]\n",
    "SHARED_ELEMENTS: [what connects them, if anything]\n",
    "JUSTIFICATION: [why this score]\"\"\"\n",
    "\n",
    "def get_topic_prompt():\n",
    "    \"\"\"Get prompt for topic identification.\"\"\"\n",
    "    return \"\"\"Analyze this document's topic and category.\n",
    "\n",
    "{text}\n",
    "\n",
    "OUTPUT:\n",
    "MAIN_TOPIC: [specific topic]\n",
    "CATEGORY: [broad category like: religion, sports, technology, politics, science, etc.]\n",
    "SUBTOPICS: [3-4 key themes]\n",
    "CONFIDENCE: [0-1 score]\"\"\"\n",
    "\n",
    "# ======================================\n",
    "# LLM PROCESSING\n",
    "# ======================================\n",
    "\n",
    "class ImprovedLLMProcessor:\n",
    "    \"\"\"Handles interaction with Language Models for document evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str, model: str = \"deepseek-r1:32b\"):\n",
    "        self.model = model\n",
    "        self.api_url = api_url\n",
    "        self.prompts = {\n",
    "            \"summarize\": get_improved_summarize_prompt(),\n",
    "            \"grade\": get_enhanced_grade_prompt(),\n",
    "            \"topic\": get_topic_prompt()\n",
    "        }\n",
    "\n",
    "    def process_text(self, text: str, task: str, additional_context: Dict = None) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Process text with the LLM for different tasks (summarize, grade, topic).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt_template = self.prompts.get(task)\n",
    "            if not prompt_template:\n",
    "                raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "            context = {\n",
    "                \"text\": text,\n",
    "                \"documents\": \"\",\n",
    "                \"other_groups\": \"\"\n",
    "            }\n",
    "            if additional_context:\n",
    "                context.update(additional_context)\n",
    "\n",
    "            prompt = prompt_template.format(**context)\n",
    "            \n",
    "            # Lower temperature for more consistent evaluation\n",
    "            temperature = 0.3 if task == \"grade\" else 0.2\n",
    "\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": 0.9,\n",
    "                \"stream\": False\n",
    "            }\n",
    "\n",
    "            # Add timeout and retry logic\n",
    "            max_retries = 3\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count < max_retries:\n",
    "                try:\n",
    "                    response = requests.post(self.api_url, json=payload, timeout=30)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    # Check if the response has the expected format\n",
    "                    json_response = response.json()\n",
    "                    \n",
    "                    # Handle different response formats\n",
    "                    if \"response\" in json_response:\n",
    "                        content = json_response[\"response\"].strip()\n",
    "                    elif \"text\" in json_response:\n",
    "                        content = json_response[\"text\"].strip()\n",
    "                    elif \"content\" in json_response:\n",
    "                        content = json_response[\"content\"].strip()\n",
    "                    elif \"output\" in json_response:\n",
    "                        content = json_response[\"output\"].strip()\n",
    "                    else:\n",
    "                        # If we can't find expected keys, just use the whole response\n",
    "                        print(f\"Unexpected API response format: {json_response.keys()}\")\n",
    "                        content = str(json_response)\n",
    "                    \n",
    "                    if task == \"grade\":\n",
    "                        score = self._extract_score(content)\n",
    "                        main_topics = self._extract_value(content, \"MAIN_TOPICS\", \"\")\n",
    "                        shared_elements = self._extract_value(content, \"SHARED_ELEMENTS\", \"\")\n",
    "                        \n",
    "                        return LLMResponse(\n",
    "                            content=content, \n",
    "                            score=score, \n",
    "                            metadata={\n",
    "                                \"main_topics\": main_topics,\n",
    "                                \"shared_elements\": shared_elements\n",
    "                            }\n",
    "                        )\n",
    "                    elif task == \"topic\":\n",
    "                        topic_info = self._extract_topic_info(content)\n",
    "                        return LLMResponse(\n",
    "                            content=content,\n",
    "                            topic=topic_info[\"main_topic\"],\n",
    "                            confidence=topic_info[\"confidence\"],\n",
    "                            metadata=topic_info\n",
    "                        )\n",
    "\n",
    "                    return LLMResponse(content=content)\n",
    "                    \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    retry_count += 1\n",
    "                    if retry_count < max_retries:\n",
    "                        print(f\"API request failed, retrying ({retry_count}/{max_retries}): {e}\")\n",
    "                        # Exponential backoff\n",
    "                        time.sleep(2 ** retry_count)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing {task}: {e}\")\n",
    "            return LLMResponse(content=\"\", error=str(e))\n",
    "\n",
    "    def _extract_score(self, response_text):\n",
    "        \"\"\"Extract coherence score from LLM response.\"\"\"\n",
    "        try:\n",
    "            # Look for COHERENCE_SCORE\n",
    "            match = re.search(r'COHERENCE_SCORE:\\s*(\\d+)', response_text)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "                \n",
    "            # Look for any number between 1-10\n",
    "            numbers = re.findall(r'\\b([1-9]|10)\\b', response_text)\n",
    "            if numbers:\n",
    "                return int(numbers[0])\n",
    "                \n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting score: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _extract_value(self, response_text, field, default=\"\"):\n",
    "        \"\"\"Extract a field value from LLM response.\"\"\"\n",
    "        try:\n",
    "            pattern = f\"{field}:\\\\s*([^\\\\n]+)\"\n",
    "            match = re.search(pattern, response_text)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "            return default\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def _extract_topic_info(self, response_text):\n",
    "        \"\"\"Extract topic information from LLM response.\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                \"main_topic\": self._extract_value(response_text, \"MAIN_TOPIC\", \"unknown\"),\n",
    "                \"category\": self._extract_value(response_text, \"CATEGORY\", \"unknown\"),\n",
    "                \"subtopics\": self._extract_value(response_text, \"SUBTOPICS\", \"\").split(','),\n",
    "                \"confidence\": float(self._extract_value(response_text, \"CONFIDENCE\", \"0.5\"))\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\n",
    "                \"main_topic\": \"unknown\",\n",
    "                \"category\": \"unknown\",\n",
    "                \"subtopics\": [],\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "\n",
    "def update_llm_processor_with_calibration(llm_processor):\n",
    "    \"\"\"Update the LLM processor with enhanced prompts.\"\"\"\n",
    "    # Update the grade prompt\n",
    "    llm_processor.prompts[\"grade\"] = get_enhanced_grade_prompt()\n",
    "    return llm_processor\n",
    "\n",
    "# ======================================\n",
    "# COHERENCE CALCULATION\n",
    "# ======================================\n",
    "\n",
    "def calculate_coherence_scores(groups, dictionary, measure=\"c_v\"):\n",
    "    \"\"\"\n",
    "    Calculate coherence scores for groups of documents.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    groups : List[List[str]]\n",
    "        List of document groups, where each group is a list of tokenized documents\n",
    "    dictionary : Dictionary\n",
    "        Gensim Dictionary object (not used in this implementation as compute_coherence creates its own)\n",
    "    measure : str, default=\"c_v\"\n",
    "        Coherence measure to use (not used as compute_coherence uses \"c_v\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[float]\n",
    "        List of coherence scores for each group\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for group in groups:\n",
    "        try:\n",
    "            # Use the compute_coherence function directly\n",
    "            score = compute_coherence(group)\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating coherence for group: {e}\")\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "    \n",
    "def compute_coherence(texts):\n",
    "    \"\"\"\n",
    "    Compute coherence score for a group of texts using Latent Dirichlet Allocation (LDA) \n",
    "    and the c_v coherence measure.\n",
    "    \n",
    "    This function quantifies how semantically coherent a group of documents are with each other.\n",
    "    It works by:\n",
    "    1. Creating a dictionary from the tokenized texts\n",
    "    2. Converting documents to bag-of-words representation\n",
    "    3. Training an LDA model with 2 topics\n",
    "    4. Computing the c_v coherence measure, which is based on normalized pointwise mutual information (NPMI)\n",
    "       and the indirect cosine measure\n",
    "    \n",
    "    Higher coherence scores indicate greater semantic similarity among the documents, suggesting\n",
    "    they discuss related topics or themes. The c_v measure typically ranges from 0 to 1, with \n",
    "    scores closer to 1 indicating stronger coherence.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : List[List[str]]\n",
    "        A list of tokenized documents, where each document is represented as a list of tokens/words.\n",
    "        The texts should already be preprocessed (tokenized, with stopwords removed).\n",
    "        Example: [['cloud', 'computing', 'infrastructure'], ['data', 'processing', 'algorithms']]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The coherence score (c_v measure) for the document group. Higher values indicate\n",
    "        greater semantic coherence between documents.\n",
    "    \"\"\"\n",
    "\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Create LDA model\n",
    "    lda = LdaModel(\n",
    "        corpus=corpus, \n",
    "        id2word=dictionary, \n",
    "        num_topics=2, \n",
    "        passes=10, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Calculate coherence\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda, \n",
    "        texts=texts, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def perform_lda_analysis(documents, n_topics=5):\n",
    "    \"\"\"\n",
    "    Performs Latent Dirichlet Allocation (LDA) topic modeling on a collection of documents.\n",
    "    \n",
    "    This function applies improved LDA analysis with robust preprocessing to extract latent topics\n",
    "    from a collection of text documents. It uses scikit-learn's CountVectorizer for text \n",
    "    preprocessing and LatentDirichletAllocation for topic modeling.\n",
    "    \n",
    "    The function automatically handles edge cases, such as insufficient features for the \n",
    "    requested number of topics, and provides informative error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : List[str]\n",
    "        A list of document texts to analyze. Each element should be a string containing\n",
    "        the text of one document.\n",
    "        \n",
    "    n_topics : int, default=5\n",
    "        The number of topics to extract using LDA. If the number of features is insufficient\n",
    "        for the requested number of topics, this value will be automatically adjusted.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing two keys:\n",
    "        - 'assigned_topics': A list of integers representing the most dominant topic\n",
    "          for each document in the input list. Each integer corresponds to the index\n",
    "          of the most probable topic for that document.\n",
    "        - 'topics_keywords': A dictionary mapping topic indices to lists of keywords.\n",
    "          Each topic is represented by its top 15 most relevant keywords.\n",
    "    \"\"\"\n",
    "    if not documents or not isinstance(documents, list) or len(documents) < n_topics:\n",
    "        return {\"assigned_topics\": [], \"topics_keywords\": {}}\n",
    "\n",
    "    try:\n",
    "        # Use minimal preprocessing\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words='english',\n",
    "            max_df=0.9,\n",
    "            min_df=0.05,\n",
    "            token_pattern=r'(?u)\\b\\w+\\b',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "        # Create document-term matrix\n",
    "        X = vectorizer.fit_transform([doc if isinstance(doc, str) else \"\" for doc in documents])\n",
    "        \n",
    "        # Check if we have enough features for LDA\n",
    "        if X.shape[1] < n_topics * 2:\n",
    "            print(f\"Warning: Not enough features ({X.shape[1]}) for {n_topics} topics\")\n",
    "            n_topics = max(2, X.shape[1] // 2)\n",
    "        \n",
    "        # Fit LDA\n",
    "        lda_model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            max_iter=50,\n",
    "            learning_method='batch',\n",
    "            learning_offset=50.0,\n",
    "            doc_topic_prior=0.1,\n",
    "            topic_word_prior=0.01\n",
    "        )\n",
    "\n",
    "        lda_model.fit(X)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Extract topics and keywords\n",
    "        topics = {}\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_keywords_idx = topic.argsort()[:-15:-1]\n",
    "            top_keywords = [feature_names[i] for i in top_keywords_idx]\n",
    "            topics[topic_idx] = top_keywords\n",
    "\n",
    "        # Get topic assignments for documents\n",
    "        topic_assignments = lda_model.transform(X)\n",
    "        assigned_topics = np.argmax(topic_assignments, axis=1)\n",
    "\n",
    "        return {\n",
    "            \"assigned_topics\": assigned_topics.tolist(),\n",
    "            \"topics_keywords\": topics\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LDA analysis: {str(e)}\")\n",
    "        return {\"assigned_topics\": [], \"topics_keywords\": {}}\n",
    "\n",
    "# ======================================\n",
    "# RAG UTILITIES\n",
    "# ======================================\n",
    "\n",
    "\n",
    "def create_coherence_calibration_rag(rag_system):\n",
    "    \"\"\"\n",
    "    Initialize the RAG system with coherence calibration examples from the GOLDEN_EXAMPLES.\n",
    "    \n",
    "    This function takes a RAG system and populates it with carefully crafted document\n",
    "    coherence examples that serve as reference points for coherence evaluation. These\n",
    "    examples act as anchors with known coherence scores (ranging from 1-10) that help\n",
    "    calibrate the evaluation process.\n",
    "    \n",
    "    For each golden example, the function:\n",
    "    1. Creates a formatted document containing the coherence score, example name,\n",
    "       document contents, and detailed explanation of why it received that score\n",
    "    2. Adds these formatted documents to the RAG system's document store\n",
    "    3. Ensures these examples can be retrieved later when evaluating new document groups\n",
    "    \n",
    "    The calibration examples cover different levels of coherence:\n",
    "    - High coherence (score 10): Documents covering the same specific topic (e.g., computing)\n",
    "    - Medium coherence (score 5): Documents with partial thematic connections but \n",
    "      covering different specific topics (e.g., sustainability across different domains)\n",
    "    - Low coherence (score 1): Documents covering completely unrelated topics with\n",
    "      no meaningful connections\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rag_system : RAGSystem\n",
    "        An initialized but empty RAG system to which calibration examples will be added\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    RAGSystem\n",
    "        The updated RAG system with calibration examples added to its document store\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Adding coherence calibration examples to RAG system...\")\n",
    "    \n",
    "    # Create documents that contain both the example and its score\n",
    "    calibration_docs = []\n",
    "    \n",
    "    for example in GOLDEN_EXAMPLES:\n",
    "        # Create a document that describes the example\n",
    "        doc_text = f\"COHERENCE EXAMPLE - Score {example['coherence_score']}/10\\n\\n\"\n",
    "        doc_text += f\"Group: {example['group_name']}\\n\\n\"\n",
    "        doc_text += \"Documents:\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(example['documents']):\n",
    "            doc_text += f\"Document {i+1}: {doc}\\n\\n\"\n",
    "        \n",
    "        doc_text += f\"Explanation: {example['explanation']}\\n\"\n",
    "        doc_text += f\"This is a reference example of coherence level {example['coherence_score']}/10.\"\n",
    "        \n",
    "        calibration_docs.append(doc_text)\n",
    "    \n",
    "    # Add the calibration documents to the RAG system\n",
    "    rag_system.add_documents(calibration_docs)\n",
    "    \n",
    "    print(f\"Added {len(calibration_docs)} calibration examples to RAG system\")\n",
    "    return rag_system\n",
    "\n",
    "def retrieve_similar_coherence_examples(rag_system, documents, top_k=2):\n",
    "    \"\"\"\n",
    "    Use the RAG system to find similar coherence examples for a group of documents.\n",
    "    \n",
    "    This function creates a query based on the provided documents and uses the RAG system\n",
    "    to retrieve the most semantically similar calibration examples. These retrieved examples\n",
    "    serve as reference points for the language model when evaluating document coherence.\n",
    "    \n",
    "    The process works as follows:\n",
    "    1. Constructs a query by combining truncated versions of each document\n",
    "    2. Uses the RAG system's vector search to find calibration examples with similar content\n",
    "    3. Returns the top-k most relevant examples based on embedding similarity\n",
    "    \n",
    "    These retrieved examples help the LLM better assess coherence by providing concrete\n",
    "    comparison points with known coherence scores. This helps ground the evaluation\n",
    "    and improves scoring consistency across different document groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rag_system : RAGSystem\n",
    "        The RAG system containing indexed calibration examples\n",
    "    documents : List[str]\n",
    "        List of document texts for which to find similar calibration examples\n",
    "    top_k : int, default=2\n",
    "        Number of similar examples to retrieve\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Document]\n",
    "        List of Document objects containing the most relevant calibration examples,\n",
    "        ordered by decreasing similarity (most similar first)\n",
    "    \"\"\"\n",
    "    # Create a query that describes the current document group\n",
    "    query = \"Find similar document groups to assess coherence:\\n\\n\"\n",
    "    \n",
    "    # Add current documents to the query\n",
    "    for i, doc in enumerate(documents):\n",
    "        query += f\"Document {i+1}: {doc[:150]}...\\n\\n\"  # Use truncated versions to keep query manageable\n",
    "    \n",
    "    # Retrieve relevant calibration examples\n",
    "    relevant_examples = rag_system.retrieve_relevant_docs(query, k=top_k)\n",
    "    \n",
    "    return relevant_examples\n",
    "\n",
    "\n",
    "def reinforce_calibration_examples(rag_system):\n",
    "    \"\"\"\n",
    "    Add additional examples to the RAG system that strongly reinforce proper coherence scoring behavior.\n",
    "    \n",
    "    This function enhances the RAG system's calibration by adding three categories of examples:\n",
    "    \n",
    "    1. Scoring guidelines - Direct instructions on how to use the full scoring range properly\n",
    "    2. Bad division examples - Examples of document groups with low coherence (scores 1-4)\n",
    "    3. Mixed division examples - Examples with moderate coherence levels (scores 5-6)\n",
    "    \n",
    "    These examples serve as additional reference points beyond the basic golden examples,\n",
    "    helping the language model better understand edge cases and apply consistent scoring.\n",
    "    The reinforcement examples specifically address common scoring biases, such as:\n",
    "    - Reluctance to use extreme scores (1-2 or 9-10)\n",
    "    - Inconsistent handling of partially related documents\n",
    "    - Failure to recognize when all documents share the same specific topic\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rag_system : RAGSystem\n",
    "        The RAG system instance to which calibration examples will be added\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    RAGSystem\n",
    "        The updated RAG system with reinforcement examples added to its document store\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Adding reinforcement calibration examples to RAG system...\")\n",
    "    \n",
    "    # Create documents that emphasize correct scoring\n",
    "    reinforcement_docs = [\n",
    "        \"SCORING GUIDE: When evaluating document coherence, you MUST use the full scoring range. Documents that are all about the same specific topic (e.g., all discussing basketball strategies) deserve scores of 9-10. Only give scores of 6-7 if documents are in the same general field but different subtopics. Documents with no connection should receive scores of 1-2.\",\n",
    "        \n",
    "        \"COHERENCE EVALUATION EXAMPLE: Documents about artificial intelligence applications, ethical concerns in AI, and advancements in NLP are all clearly about the same specific topic (Artificial Intelligence). This group demonstrates high coherence and must receive a score of 9-10.\",\n",
    "        \n",
    "        \"COHERENCE EVALUATION EXAMPLE: Documents discussing basketball offensive strategies, basketball defensive evolution, and basketball player development are all explicitly about the same specific domain (Basketball). This group demonstrates high coherence and must receive a score of 9-10.\"\n",
    "    ]\n",
    "    \n",
    "    # Add examples of BAD DIVISIONS with low coherence scores\n",
    "    bad_division_examples = [\n",
    "        \"\"\"COHERENCE EXAMPLE - Score 1/10\n",
    "        \n",
    "        Group: Completely Unrelated Topics\n",
    "        \n",
    "        Documents:\n",
    "        Document 1: Climate change is accelerating with global temperatures rising at an unprecedented rate. Arctic ice melt and extreme weather events are among the most visible impacts currently affecting communities worldwide.\n",
    "        \n",
    "        Document 2: The history of classical music in Vienna during the 18th century was dominated by composers like Mozart and Haydn who established many of the formal structures still used in orchestral composition today.\n",
    "        \n",
    "        Document 3: Cryptocurrency mining operations require significant computational resources and energy consumption, raising concerns about their environmental impact and long-term sustainability.\n",
    "        \n",
    "        Document 4: Traditional cake recipes often include flour, sugar, eggs, and butter as base ingredients, with variations in proportions and additional flavorings determining the specific type of cake produced.\n",
    "        \n",
    "        Document 5: Ancient Egyptian burial practices involved elaborate preservation techniques for the deceased, including mummification and the construction of tombs filled with artifacts believed necessary for the afterlife.\n",
    "        \n",
    "        Explanation: These documents have absolutely no topical connection to each other, covering climate science, music history, cryptocurrency, baking, and archaeology. They share no vocabulary, themes, or concepts. This represents the lowest level of coherence with completely unrelated content.\n",
    "        This is a reference example of coherence level 1/10.\"\"\",\n",
    "\n",
    "        \"\"\"COHERENCE EXAMPLE - Score 3/10\n",
    "        \n",
    "        Group: Mostly Disconnected with Minimal Overlap\n",
    "        \n",
    "        Documents:\n",
    "        Document 1: Recent advances in artificial intelligence have enabled more accurate weather prediction models that can forecast severe storms up to 5 days in advance.\n",
    "        \n",
    "        Document 2: The global semiconductor shortage has severely impacted automotive production, with many manufacturers unable to complete vehicles due to missing electronic components.\n",
    "        \n",
    "        Document 3: Smart farming technologies using IoT sensors can monitor soil moisture and automatically adjust irrigation systems to conserve water while improving crop yields.\n",
    "        \n",
    "        Document 4: The rising cost of housing in urban centers has forced many families to commute longer distances from affordable suburban areas to their workplaces.\n",
    "        \n",
    "        Document 5: Online learning platforms experienced unprecedented growth during the pandemic as schools and universities transitioned to remote education models.\n",
    "        \n",
    "        Explanation: While these documents all relate to modern developments, they address entirely different domains (weather forecasting, manufacturing, agriculture, housing, and education). There is minimal conceptual overlap with only a loose connection through technology references in some documents. Most documents have no meaningful relationship to the others. This represents very low coherence.\n",
    "        This is a reference example of coherence level 3/10.\"\"\",\n",
    "        \n",
    "        \"\"\"COHERENCE EXAMPLE - Score 4/10\n",
    "        \n",
    "        Group: Weakly Connected Topics\n",
    "        \n",
    "        Documents:\n",
    "        Document 1: The European Union's carbon tax legislation aims to reduce greenhouse gas emissions by putting a price on carbon-intensive industrial production.\n",
    "        \n",
    "        Document 2: Electric vehicles are becoming increasingly popular in urban centers where charging infrastructure is more developed and commute distances are shorter.\n",
    "        \n",
    "        Document 3: Corporate social responsibility reports now commonly include detailed sustainability metrics including waste reduction and energy efficiency measures.\n",
    "        \n",
    "        Document 4: The global fashion industry faces criticism for its environmental impact, including water pollution from textile manufacturing and the short lifecycle of fast fashion products.\n",
    "        \n",
    "        Document 5: Advances in quantum computing research focus primarily on theoretical applications rather than immediate commercial deployment due to the technical challenges involved.\n",
    "        \n",
    "        Explanation: While four documents share loose connections to environmental themes, they address different industries and aspects (legislation, transportation, corporate reporting, and fashion). The fifth document about quantum computing is entirely unrelated to this loose environmental theme. This group demonstrates low coherence with some weak connections between most documents but still lacks a unified topic.\n",
    "        This is a reference example of coherence level 4/10.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Add more specific examples showing mixed divisions with varied scores\n",
    "    mixed_division_examples = [\n",
    "        \"\"\"COHERENCE EXAMPLE - Score 5/10\n",
    "        \n",
    "        Group: Mixed Topics with Some Thematic Connection\n",
    "        \n",
    "        Documents:\n",
    "        Document 1: Major smartphone manufacturers release new models annually, with incremental hardware improvements and software features to entice consumers to upgrade.\n",
    "        \n",
    "        Document 2: Wearable fitness trackers can monitor heart rate, sleep patterns, and activity levels, providing users with health insights through connected mobile applications.\n",
    "        \n",
    "        Document 3: Social media platforms use algorithmic content curation to maximize user engagement, which has raised concerns about filter bubbles and information diversity.\n",
    "        \n",
    "        Document 4: The rise of streaming services has transformed how television content is produced and consumed, with binge-watching becoming a common viewing habit.\n",
    "        \n",
    "        Document 5: Digital privacy regulations like GDPR have forced technology companies to revise their data collection and storage practices globally.\n",
    "        \n",
    "        Explanation: These documents all relate broadly to consumer technology and digital trends, but address different specific areas (smartphones, wearables, social media, streaming entertainment, and privacy regulation). They share some vocabulary and conceptual overlap through digital technology, but each focuses on a different aspect with distinct concerns. This represents moderate coherence with a loose connecting theme but no single specific topic.\n",
    "        This is a reference example of coherence level 5/10.\"\"\",\n",
    "        \n",
    "        \"\"\"COHERENCE EXAMPLE - Score 6/10\n",
    "        \n",
    "        Group: Related Topics with Stronger Connections\n",
    "        \n",
    "        Documents:\n",
    "        Document 1: Machine learning algorithms require large training datasets to achieve high accuracy in pattern recognition tasks.\n",
    "        \n",
    "        Document 2: Computer vision systems can now identify objects in images with near-human accuracy when properly trained on diverse visual data.\n",
    "        \n",
    "        Document 3: Natural language processing has improved significantly with the development of transformer models that better understand contextual relationships between words.\n",
    "        \n",
    "        Document 4: The ethical implications of algorithmic decision-making include concerns about bias, transparency, and accountability in automated systems.\n",
    "        \n",
    "        Document 5: Recent developments in robotics focus on improving sensory capabilities to allow machines to navigate complex environments more effectively.\n",
    "        \n",
    "        Explanation: These documents all relate to artificial intelligence and its applications, with each addressing different aspects of the field (general machine learning, computer vision, NLP, ethics, and robotics). They share technical vocabulary and conceptual frameworks while maintaining distinct focuses. This represents good coherence with a clear general field but variation in specific subtopics.\n",
    "        This is a reference example of coherence level 6/10.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Add everything to our collection of examples\n",
    "    reinforcement_docs.extend(bad_division_examples)\n",
    "    reinforcement_docs.extend(mixed_division_examples)\n",
    "    \n",
    "    # Add these reinforcement documents to the RAG system\n",
    "    rag_system.add_documents(reinforcement_docs)\n",
    "    \n",
    "    print(f\"Added {len(reinforcement_docs)} reinforcement examples to RAG system\")\n",
    "    return rag_system\n",
    "\n",
    "# ======================================\n",
    "# SMOOTHING & EVALUATION\n",
    "# ======================================\n",
    "\n",
    "def smooth_coherence_scores(scores, threshold=2.5, expected_score=None, mixing_pattern=None):\n",
    "    \"\"\"\n",
    "    Smooth coherence scores to handle outliers and inconsistencies.\n",
    "    \n",
    "    Parameters:\n",
    "    - scores: List of coherence scores from multiple iterations\n",
    "    - threshold: Maximum allowed difference between scores before applying smoothing\n",
    "    - expected_score: Optional expected score based on mix pattern (if known)\n",
    "    - mixing_pattern: String describing the document mix pattern (e.g., \"5 same\", \"10 same\")\n",
    "    \n",
    "    Returns:\n",
    "    - Smoothed average score\n",
    "    \"\"\"\n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    \n",
    "    if len(scores) == 1:\n",
    "        return scores[0]\n",
    "    \n",
    "    # Calculate the range (max - min)\n",
    "    score_range = max(scores) - min(scores)\n",
    "    \n",
    "    # If scores are close enough, use simple average\n",
    "    if score_range <= threshold:\n",
    "        return sum(scores) / len(scores)\n",
    "    \n",
    "    # If scores have high variance, apply smoothing\n",
    "    print(f\"  High variance detected in scores {scores}, applying smoothing...\")\n",
    "    \n",
    "    # If mixing pattern is provided but expected score isn't, infer it\n",
    "    if expected_score is None and mixing_pattern is not None:\n",
    "        if \"10 same\" in mixing_pattern or \"all same\" in mixing_pattern.lower():\n",
    "            expected_score = 10.0  # Expect high coherence for same category\n",
    "        elif \"all different\" in mixing_pattern.lower() or \"0 same\" in mixing_pattern:\n",
    "            expected_score = 1.0  # Expect low coherence for different categories\n",
    "        elif \"8 same\" in mixing_pattern:\n",
    "            expected_score = 8.0  # Mostly same\n",
    "        elif \"6 same\" in mixing_pattern:\n",
    "            expected_score = 6.0  # Mixed\n",
    "        elif \"4 same\" in mixing_pattern:\n",
    "            expected_score = 4.0  # Mostly different\n",
    "        elif \"5 same\" in mixing_pattern:\n",
    "            expected_score = 10.0  # Expect high coherence for same category \n",
    "        elif \"4 same\" in mixing_pattern:\n",
    "            expected_score = 8.0  # Mostly same\n",
    "        elif \"3 same\" in mixing_pattern:\n",
    "            expected_score = 6.0  # Mixed\n",
    "        elif \"2 same\" in mixing_pattern:\n",
    "            expected_score = 4.0  # Mostly different\n",
    "        elif \"0 same\" in mixing_pattern or \"all different\" in mixing_pattern.lower():\n",
    "            expected_score = 1.0  # Expect low coherence for different categories\n",
    "    \n",
    "    # Apply different smoothing strategies based on available information\n",
    "    if expected_score is not None:\n",
    "        # If we have an expected score, weight by distance from expected\n",
    "        weights = [1 / (abs(score - expected_score) + 0.5) for score in scores]\n",
    "        total_weight = sum(weights)\n",
    "        smoothed = sum(score * weight for score, weight in zip(scores, weights)) / total_weight\n",
    "        print(f\"  Used expected-score smoothing: {smoothed:.2f} (expected {expected_score})\")\n",
    "    else:\n",
    "        # Without expected score, use a robust average:\n",
    "        \n",
    "        # Option 1: Median (good for 3+ scores)\n",
    "        if len(scores) >= 3:\n",
    "            # Sort scores and take the middle one\n",
    "            smoothed = sorted(scores)[len(scores) // 2]\n",
    "            print(f\"  Used median smoothing: {smoothed:.2f}\")\n",
    "        \n",
    "        # Option 2: Winsorized mean (clip outliers to within threshold of other scores)\n",
    "        else:\n",
    "            # For two scores with high variance, use the strategy:\n",
    "            # 1. If one score is unusually low (< 2) and other is high (> 7), prefer the higher score\n",
    "            if (min(scores) < 2.0 and max(scores) > 7.0):\n",
    "                smoothed = max(scores) * 0.9  # Slightly discount the higher score\n",
    "                print(f\"  Used high-score preference smoothing: {smoothed:.2f}\")\n",
    "            else:\n",
    "                # Otherwise use winsorized mean\n",
    "                mean = sum(scores) / len(scores)\n",
    "                clipped_scores = [\n",
    "                    min(max(score, mean - threshold), mean + threshold)\n",
    "                    for score in scores\n",
    "                ]\n",
    "                smoothed = sum(clipped_scores) / len(clipped_scores)\n",
    "                print(f\"  Used winsorized mean smoothing: {smoothed:.2f}\")\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "def evaluate_with_rag_assistance(groups, topics, llm_processor, rag_system, num_iterations=2, skip_summarization=False):\n",
    "    \"\"\"\n",
    "    Enhanced evaluation function that uses RAG to assist with coherence assessment.\n",
    "    \n",
    "    This function evaluates the coherence of groups of documents using both computational methods\n",
    "    and language model (LLM) assessment with RAG assistance. The function:\n",
    "    \n",
    "    1. Initializes the RAG system with calibration examples if needed\n",
    "    2. Optionally summarizes documents to extract key information\n",
    "    3. Calculates computational coherence scores using topic modeling\n",
    "    4. Performs LDA analysis to identify primary topics in each group\n",
    "    5. Uses an LLM to evaluate coherence with multiple iterations for reliability\n",
    "    6. Retrieves similar examples from the RAG system to provide context for LLM evaluation\n",
    "    7. Calculates and returns comprehensive coherence metrics and analyses\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    groups : List[List[str]]\n",
    "        List of document groups, where each group is a list of document strings\n",
    "    topics : List[str]\n",
    "        List of topic labels corresponding to each document group\n",
    "    llm_processor : ImprovedLLMProcessor\n",
    "        Instance of LLM processor to use for text evaluation \n",
    "    rag_system : RAGSystem\n",
    "        Instance of RAG system for retrieving similar coherence examples\n",
    "    num_iterations : int, default=2\n",
    "        Number of iterations for LLM coherence scoring (higher = more reliable)\n",
    "    skip_summarization : bool, default=False\n",
    "        If True, uses original documents without summarization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing comprehensive evaluation results including:\n",
    "        - LLM coherence scores for each topic\n",
    "        - LLM analysis details (identified topics, shared elements)\n",
    "        - Computational coherence scores\n",
    "        - LDA topic analysis results\n",
    "        - Document samples used for evaluation\n",
    "        - RAG examples retrieved for context\n",
    "        - Summary statistics for each topic\n",
    "    \"\"\"\n",
    "    # Initialize RAG with calibration examples if not already done\n",
    "    if not rag_system.document_store:\n",
    "        rag_system = create_coherence_calibration_rag(rag_system)\n",
    "    \n",
    "    results = {\n",
    "        'llm_scores': {topic: [] for topic in topics},\n",
    "        'llm_analysis': {topic: [] for topic in topics},\n",
    "        'coherence_scores': {topic: [] for topic in topics},\n",
    "        'lda_results': {topic: None for topic in topics},\n",
    "        'document_samples': {topic: [] for topic in topics},\n",
    "        'rag_examples': {topic: [] for topic in topics}  # Store which examples were retrieved\n",
    "    }\n",
    "    \n",
    "    # Skip summarization if requested\n",
    "    if skip_summarization:\n",
    "        print(\"Using original documents (skipping summarization)...\")\n",
    "        summarized_groups = groups\n",
    "    else:\n",
    "        # Summarize documents\n",
    "        print(\"Summarizing documents...\")\n",
    "        summarized_groups = []\n",
    "        for i, group in enumerate(groups):\n",
    "            print(f\"Summarizing group {i+1}/{len(groups)}: {topics[i]}\")\n",
    "            \n",
    "            # Take a sample of documents\n",
    "            subset_size = min(5, len(group))\n",
    "            if len(group) > subset_size:\n",
    "                np.random.seed(42)\n",
    "                subset_indices = np.random.choice(len(group), subset_size, replace=False)\n",
    "                group_subset = [group[i] for i in subset_indices]\n",
    "            else:\n",
    "                group_subset = group\n",
    "            \n",
    "            # Store document samples\n",
    "            results['document_samples'][topics[i]] = [doc[:200] for doc in group_subset]\n",
    "                \n",
    "            # Summarize the documents\n",
    "            summarized = []\n",
    "            for doc in group_subset:\n",
    "                try:\n",
    "                    response = llm_processor.process_text(doc, \"summarize\")\n",
    "                    if response and not response.error and response.content:\n",
    "                        # Extract summary properly\n",
    "                        summary_match = re.search(r'SUMMARY:\\s*([^\\n]+)', response.content)\n",
    "                        if summary_match:\n",
    "                            summary = summary_match.group(1).strip()\n",
    "                        else:\n",
    "                            summary = response.content[:300]\n",
    "                        summarized.append(summary)\n",
    "                    else:\n",
    "                        # If summarization fails, use original document\n",
    "                        summarized.append(doc[:500] + \"...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Summarization error: {e}\")\n",
    "                    summarized.append(doc[:500] + \"...\")\n",
    "            summarized_groups.append(summarized)\n",
    "    \n",
    "    # Preprocess all groups for coherence calculation\n",
    "    print(\"Preprocessing documents...\")\n",
    "    tokenized_groups = []\n",
    "    for group in groups:\n",
    "        tokenized_group = []\n",
    "        for doc in group:\n",
    "            if isinstance(doc, str):\n",
    "                # Use the preprocess function\n",
    "                tokens = preprocess(doc)\n",
    "                if tokens:  # Only append if we have tokens\n",
    "                    tokenized_group.append(tokens)\n",
    "                else:\n",
    "                    tokenized_group.append(['placeholder'])\n",
    "            else:\n",
    "                tokenized_group.append(['placeholder'])\n",
    "        tokenized_groups.append(tokenized_group)\n",
    "    \n",
    "    # Create dictionary from all documents\n",
    "    all_docs_tokenized = [token for group in tokenized_groups for token in group]\n",
    "    dictionary = Dictionary(all_docs_tokenized)\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.95)\n",
    "    \n",
    "    # Calculate coherence scores\n",
    "    print(\"Calculating coherence scores...\")\n",
    "    coherence_scores = calculate_coherence_scores(tokenized_groups, dictionary)\n",
    "    for topic, score in zip(topics, coherence_scores):\n",
    "        results['coherence_scores'][topic] = [score]\n",
    "        \n",
    "    # Perform LDA analysis on each group\n",
    "    print(\"Performing LDA analysis...\")\n",
    "    for i, (group, topic) in enumerate(zip(groups, topics)):\n",
    "        n_topics = min(3, max(2, len(group) // 2))\n",
    "        lda_result = perform_lda_analysis(group, n_topics=n_topics)\n",
    "        results['lda_results'][topic] = lda_result\n",
    "    \n",
    "    # LLM evaluations with RAG assistance\n",
    "    if llm_processor:\n",
    "        for i in range(num_iterations):\n",
    "            print(f\"\\nIteration {i + 1}/{num_iterations}\")\n",
    "            print(\"Performing LLM evaluation with RAG assistance...\")\n",
    "            \n",
    "            for j, (group, topic) in enumerate(zip(summarized_groups, topics)):\n",
    "                # Skip groups that are too small for meaningful evaluation\n",
    "                if len(group) < 2:\n",
    "                    print(f\"Skipping {topic} (too few documents)\")\n",
    "                    continue\n",
    "                    \n",
    "                # Get similar coherence examples from RAG\n",
    "                similar_examples = retrieve_similar_coherence_examples(rag_system, group)\n",
    "                results['rag_examples'][topic] = [ex.content for ex in similar_examples]\n",
    "                \n",
    "                # Format the documents with clear separation\n",
    "                formatted_docs = []\n",
    "                for idx, doc in enumerate(group):\n",
    "                    formatted_docs.append(f\"DOCUMENT {idx+1}:\\n{doc}\")\n",
    "                docs_text = \"\\n\\n\".join(formatted_docs)\n",
    "                \n",
    "                # Add RAG-retrieved examples to the evaluation context\n",
    "                rag_context = \"\"\n",
    "                if similar_examples:\n",
    "                    rag_context = \"\\n\\nSIMILAR REFERENCE EXAMPLES FROM DATABASE:\\n\"\n",
    "                    for ex_idx, example in enumerate(similar_examples):\n",
    "                        rag_context += f\"Example {ex_idx+1}:\\n{example.content}\\n\\n\"\n",
    "                \n",
    "                # Get LLM evaluation with the enhanced context\n",
    "                try:\n",
    "                    response = llm_processor.process_text(\"\", \"grade\", {\n",
    "                        \"documents\": docs_text + rag_context\n",
    "                    })\n",
    "                    \n",
    "                    if response and not response.error:\n",
    "                        results['llm_scores'][topic].append(response.score)\n",
    "                        results['llm_analysis'][topic].append({\n",
    "                            'score': response.score,\n",
    "                            'main_topics': response.metadata.get('main_topics', ''),\n",
    "                            'shared_elements': response.metadata.get('shared_elements', ''),\n",
    "                            'full_response': response.content\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  Evaluated {topic}: Score = {response.score}\")\n",
    "                        print(f\"    Topics: {response.metadata.get('main_topics', 'N/A')}\")\n",
    "                        print(f\"    Retrieved {len(similar_examples)} similar examples from RAG\")\n",
    "                    else:\n",
    "                        print(f\"Error in LLM evaluation for {topic}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception during evaluation of {topic}: {e}\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results['summary'] = {}\n",
    "    for topic in topics:\n",
    "        summary = {}\n",
    "        \n",
    "        # Coherence score summary\n",
    "        if results['coherence_scores'][topic]:\n",
    "            coherence_values = results['coherence_scores'][topic]\n",
    "            summary['coherence_score'] = {\n",
    "                'value': coherence_values[0] if coherence_values else 0,\n",
    "                'normalized': 1 + 9 * coherence_values[0] if coherence_values else 0\n",
    "            }\n",
    "        \n",
    "        # LLM score summary\n",
    "        if results['llm_scores'][topic]:\n",
    "            llm_scores = results['llm_scores'][topic]\n",
    "            summary['llm_score'] = {\n",
    "                'average': sum(llm_scores) / len(llm_scores),\n",
    "                'all_scores': llm_scores,\n",
    "                'identified_topics': [\n",
    "                    analysis.get('main_topics', '') \n",
    "                    for analysis in results['llm_analysis'][topic]\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        results['summary'][topic] = summary\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ======================================\n",
    "# UTILITY FUNCTIONS FOR EXPERIMENT\n",
    "# ======================================\n",
    "\n",
    "def get_mixed_dataset(newsgroups, categories, docs_per_category=1):\n",
    "    \"\"\"\n",
    "    Get a mixed dataset with documents from multiple categories.\n",
    "    \"\"\"\n",
    "    mixed_docs = []\n",
    "    for category in categories:\n",
    "        print(f\"  - {category}: {docs_per_category} document\")\n",
    "        category_indices = [i for i in range(len(newsgroups.target))\n",
    "                        if newsgroups.target_names[newsgroups.target[i]] == category]\n",
    "        if category_indices:\n",
    "            mixed_docs.append(newsgroups.data[category_indices[0]])\n",
    "    return mixed_docs\n",
    "\n",
    "# ======================================\n",
    "# MAIN EXECUTION\n",
    "# ======================================\n",
    "\n",
    "# Main function to run 5 runs of experiments, each with 5 teams of 5 documents\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure to set your API URL for LLM before running\n",
    "    url = url  # Replace with your actual API URL\n",
    "    \n",
    "    # Download NLTK resources if not already downloaded\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK resources...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "    # Set to False if you don't want separate files for each team\n",
    "    SAVE_INDIVIDUAL_TEAM_RESULTS = True\n",
    "    \n",
    "    # Initial setup - load and preprocess data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COHERENCE EXPERIMENTS - SETUP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nLoading dataset...\")\n",
    "    # Load the CSV dataset\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(\"event2012.csv\")\n",
    "        print(f\"Loaded {len(df)} rows from event2012.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Clean the text data\n",
    "    print(\"Preprocessing text data...\")\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    df = df[df['clean_text'].str.len() > 20]  # Keep only texts with more than 20 chars\n",
    "    print(f\"After cleaning: {len(df)} rows\")\n",
    "    \n",
    "    # Add the tokenization step\n",
    "    print(\"Tokenizing text data...\")\n",
    "    df['tokens'] = df['clean_text'].apply(preprocess)\n",
    "    print(f\"Tokenization complete for {len(df)} documents\")\n",
    "    \n",
    "    # Get label distribution\n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(\"\\nLabel distribution (top 15):\")\n",
    "    for label, count in label_counts.iloc[:15].items():\n",
    "        print(f\"  Label {label}: {count} documents ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Get labels with enough documents for our experiment (at least 10 docs)\n",
    "    # We need at least 10 because we'll sample 5 per label and want some buffer\n",
    "    excluded_labels = [8,11,157]\n",
    "    valid_labels = [label for label, count in label_counts.items() \n",
    "                   if count >= 10 and label not in excluded_labels]\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_labels)} valid labels with at least 10 documents\")\n",
    "    \n",
    "    # Check if we have enough valid labels for 5 teams\n",
    "    if len(valid_labels) < 5:\n",
    "        print(f\"Warning: Not enough valid labels ({len(valid_labels)}). Need at least 5.\")\n",
    "        # Fallback if we don't have enough\n",
    "        num_primary_labels = len(valid_labels)\n",
    "        print(f\"Will use {num_primary_labels} primary labels\")\n",
    "        selected_labels = valid_labels[:num_primary_labels]\n",
    "    else:\n",
    "        # Select 5 primary labels for our experiment teams\n",
    "        selected_labels = valid_labels[:5]\n",
    "    \n",
    "    print(\"\\nSelected primary labels for teams:\")\n",
    "    for label in selected_labels:\n",
    "        count = label_counts[label]\n",
    "        print(f\"  Label {label}: {count} documents\")\n",
    "    \n",
    "    # Dictionary to store documents for each selected label\n",
    "    label_docs = {}\n",
    "    for label in selected_labels:\n",
    "        docs = df[df['label'] == label]['clean_text'].tolist()\n",
    "        # Sample 5 documents (or all if less than 5)\n",
    "        sample_size = min(5, len(docs))\n",
    "        label_docs[label] = random.sample(docs, sample_size)\n",
    "        print(f\"  Sampled {sample_size} documents for Label {label}\")\n",
    "    \n",
    "    # Define the 5 experiment runs based on mixing patterns\n",
    "    experiment_runs = [\n",
    "        {\n",
    "            \"name\": \"Run 1: All Same Category\",\n",
    "            \"description\": \"5 documents from the same category for each team\",\n",
    "            \"mixing_pattern\": \"5 same\",\n",
    "            \"doc_counts\": {\"same\": 5, \"different\": 0},\n",
    "            \"expected_score\": 10.0  # High coherence expected\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Run 2: Mostly Same Category\",\n",
    "            \"description\": \"4 documents from same category, 1 from different for each team\",\n",
    "            \"mixing_pattern\": \"4 same + 1 different\",\n",
    "            \"doc_counts\": {\"same\": 4, \"different\": 1},\n",
    "            \"expected_score\": 8.0  # Fairly high coherence expected\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Run 3: Mixed Categories\",\n",
    "            \"description\": \"3 documents from same category, 2 from different for each team\",\n",
    "            \"mixing_pattern\": \"3 same + 2 different\", \n",
    "            \"doc_counts\": {\"same\": 3, \"different\": 2},\n",
    "            \"expected_score\": 6.0  # Moderate coherence expected\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Run 4: Mostly Different\",\n",
    "            \"description\": \"2 documents from same category, 3 from different for each team\",\n",
    "            \"mixing_pattern\": \"2 same + 3 different\",\n",
    "            \"doc_counts\": {\"same\": 2, \"different\": 3},\n",
    "            \"expected_score\": 4.0  # Lower coherence expected\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Run 5: All Different\",\n",
    "            \"description\": \"0 documents from same category, 5 from different for each team\",\n",
    "            \"mixing_pattern\": \"0 same + 5 different\",\n",
    "            \"doc_counts\": {\"same\": 0, \"different\": 5},\n",
    "            \"expected_score\": 1.0  # Low coherence expected\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create a directory for results\n",
    "    results_dir = f\"coherence_experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the experiment configuration\n",
    "    with open(f\"{results_dir}/experiment_config.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"primary_labels\": [int(l) for l in selected_labels],\n",
    "            \"runs\": [\n",
    "                {\n",
    "                    \"name\": run[\"name\"],\n",
    "                    \"description\": run[\"description\"],\n",
    "                    \"mixing_pattern\": run[\"mixing_pattern\"],\n",
    "                    \"doc_counts\": run[\"doc_counts\"],\n",
    "                    \"expected_score\": run.get(\"expected_score\")\n",
    "                } for run in experiment_runs\n",
    "            ]\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Run each experiment run\n",
    "    all_results = {}\n",
    "    \n",
    "    for run_idx, run_config in enumerate(experiment_runs):\n",
    "        run_name = run_config[\"name\"]\n",
    "        same_count = run_config[\"doc_counts\"][\"same\"]\n",
    "        diff_count = run_config[\"doc_counts\"][\"different\"]\n",
    "        expected_score = run_config.get(\"expected_score\")\n",
    "        mixing_pattern = run_config[\"mixing_pattern\"]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"RUNNING {run_name}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Mixing pattern: {mixing_pattern}\")\n",
    "        print(f\"Document counts: {same_count} from same category, {diff_count} from different categories\")\n",
    "        print(f\"Expected coherence score: {expected_score if expected_score else 'Not specified'}\")\n",
    "        \n",
    "        # Initialize the RAG system and LLM processor for this run\n",
    "        print(\"\\nInitializing RAG system and LLM processor...\")\n",
    "        rag_system = RAGSystem(embedding_model=\"all-MiniLM-L6-v2\")\n",
    "        rag_system = create_coherence_calibration_rag(rag_system)\n",
    "        rag_system = reinforce_calibration_examples(rag_system)\n",
    "        llm_processor = ImprovedLLMProcessor(url)\n",
    "        llm_processor = update_llm_processor_with_calibration(llm_processor)\n",
    "        \n",
    "        # Create teams for this run\n",
    "        run_groups = []\n",
    "        run_group_names = []\n",
    "        run_group_compositions = []\n",
    "        run_group_expected_scores = []\n",
    "        \n",
    "        for team_idx, primary_label in enumerate(selected_labels):\n",
    "            # Create a group with the specified mix\n",
    "            team_docs = []\n",
    "            composition = f\"Team {team_idx+1:02d} (Primary: Label {primary_label}): \"\n",
    "            \n",
    "            # Add documents from the same category (primary label)\n",
    "            if same_count > 0:\n",
    "                team_docs.extend(label_docs[primary_label][:same_count])\n",
    "                composition += f\"{same_count} from L{primary_label}\"\n",
    "            \n",
    "            # Add documents from different categories\n",
    "            if diff_count > 0:\n",
    "                if same_count > 0:\n",
    "                    composition += \", \"\n",
    "                \n",
    "                # For different categories, use documents from other primary labels\n",
    "                # This ensures we're really mixing different topics\n",
    "                different_labels = [l for l in selected_labels if l != primary_label]\n",
    "                \n",
    "                # If we don't have enough different labels, we'll need to reuse some\n",
    "                if len(different_labels) < diff_count:\n",
    "                    # Repeat the list as needed\n",
    "                    different_labels = (different_labels * ((diff_count // len(different_labels)) + 1))[:diff_count]\n",
    "                \n",
    "                diff_docs_added = 0\n",
    "                composition += f\"{diff_count} from: \"\n",
    "                \n",
    "                for i, diff_label in enumerate(different_labels[:diff_count]):\n",
    "                    # Get a document from this label that hasn't been used in this team yet\n",
    "                    available_docs = [doc for doc in label_docs[diff_label] \n",
    "                                     if doc not in team_docs]\n",
    "                    \n",
    "                    if available_docs:\n",
    "                        doc_to_add = available_docs[0]\n",
    "                        team_docs.append(doc_to_add)\n",
    "                        diff_docs_added += 1\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            composition += f\"L{diff_label}\"\n",
    "                        elif i == diff_count - 1 or i >= 2:  # Only list up to 3 labels\n",
    "                            composition += f\", L{diff_label}\"\n",
    "                        else:\n",
    "                            composition += f\", L{diff_label}\"\n",
    "                    \n",
    "                    if i >= 2 and i < diff_count - 1:\n",
    "                        composition += \", ...\"\n",
    "                        break\n",
    "                \n",
    "                # If we couldn't get enough different documents, add some from the primary label\n",
    "                if diff_docs_added < diff_count:\n",
    "                    missing = diff_count - diff_docs_added\n",
    "                    team_docs.extend(label_docs[primary_label][same_count:same_count+missing])\n",
    "                    composition += f\" (had to reuse {missing} docs from L{primary_label})\"\n",
    "            \n",
    "            run_groups.append(team_docs)\n",
    "            run_group_names.append(f\"Team {team_idx+1:02d} (Label {primary_label})\")\n",
    "            run_group_compositions.append(composition)\n",
    "            run_group_expected_scores.append(expected_score)\n",
    "            \n",
    "            print(f\"\\nCreated {composition}\")\n",
    "            print(f\"  Team has {len(team_docs)} documents\")\n",
    "            for doc_idx, doc in enumerate(team_docs[:3]):  # Show first 3 docs\n",
    "                print(f\"  Doc {doc_idx+1}: {doc[:100]}...\")\n",
    "            if len(team_docs) > 3:\n",
    "                print(f\"  ... and {len(team_docs)-3} more documents\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(f\"\\nEvaluating all teams for {run_name}...\")\n",
    "        results = evaluate_with_rag_assistance(\n",
    "            groups=run_groups,\n",
    "            topics=run_group_names,\n",
    "            llm_processor=llm_processor,\n",
    "            rag_system=rag_system,\n",
    "            num_iterations=3,  # Three iterations for more data points\n",
    "            skip_summarization=False\n",
    "        )\n",
    "        \n",
    "        # Extract and store results\n",
    "        run_results = {}\n",
    "        comp_scores = []\n",
    "        llm_scores = []\n",
    "        llm_raw_scores = []  # Store raw (unsmoothed) scores too\n",
    "        \n",
    "        print(\"\\nTEAM RESULTS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for team_idx, (team_name, composition) in enumerate(zip(run_group_names, run_group_compositions)):\n",
    "            if team_name in results['summary']:\n",
    "                comp_score = results['summary'][team_name]['coherence_score']['normalized']\n",
    "                raw_comp_score = results['summary'][team_name]['coherence_score']['value']\n",
    "                llm_scores_all = results['summary'][team_name]['llm_score']['all_scores']\n",
    "                topics = results['summary'][team_name]['llm_score']['identified_topics']\n",
    "                \n",
    "                # Apply smoothing to the LLM scores\n",
    "                smoothed_llm_score = smooth_coherence_scores(\n",
    "                    llm_scores_all, \n",
    "                    threshold=2.5, \n",
    "                    expected_score=run_group_expected_scores[team_idx],\n",
    "                    mixing_pattern=mixing_pattern\n",
    "                )\n",
    "                \n",
    "                # Calculate raw average for comparison\n",
    "                raw_llm_avg = sum(llm_scores_all) / len(llm_scores_all) if llm_scores_all else 0\n",
    "                \n",
    "                comp_scores.append(comp_score)\n",
    "                llm_scores.append(smoothed_llm_score)\n",
    "                llm_raw_scores.append(raw_llm_avg)\n",
    "                \n",
    "                # Print team results\n",
    "                print(f\"\\n{team_name}:\")\n",
    "                print(f\"Composition: {composition}\")\n",
    "                print(f\"Computational coherence score: {comp_score:.2f} (raw: {raw_comp_score:.4f})\")\n",
    "                print(f\"LLM raw scores: {llm_scores_all}\")\n",
    "                print(f\"LLM raw average: {raw_llm_avg:.2f}\")\n",
    "                print(f\"LLM smoothed score: {smoothed_llm_score:.2f}\")\n",
    "                if abs(smoothed_llm_score - raw_llm_avg) > 0.01:  # Only show if different\n",
    "                    print(f\"  Smoothing applied: {raw_llm_avg:.2f} → {smoothed_llm_score:.2f}\")\n",
    "                \n",
    "                print(f\"Identified topics: {topics[0] if topics else 'None'}\")\n",
    "                \n",
    "                # Store team results\n",
    "                run_results[team_name] = {\n",
    "                    \"composition\": composition,\n",
    "                    \"computational_coherence\": {\n",
    "                        \"raw\": float(raw_comp_score),\n",
    "                        \"normalized\": float(comp_score)\n",
    "                    },\n",
    "                    \"llm_coherence\": {\n",
    "                        \"raw_scores\": [float(s) for s in llm_scores_all],\n",
    "                        \"raw_average\": float(raw_llm_avg),\n",
    "                        \"smoothed_score\": float(smoothed_llm_score),\n",
    "                        \"smoothing_applied\": abs(smoothed_llm_score - raw_llm_avg) > 0.01,\n",
    "                        \"topics\": topics\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Save individual team results if enabled\n",
    "                if SAVE_INDIVIDUAL_TEAM_RESULTS:\n",
    "                    with open(f\"{results_dir}/run_{run_idx+1}_team_{team_idx+1:02d}_results.json\", \"w\") as f:\n",
    "                        json.dump(run_results[team_name], f, indent=2)\n",
    "        \n",
    "        # Add a summary table for all teams in this run\n",
    "        print(\"\\nSUMMARY OF ALL TEAMS IN RUN:\")\n",
    "        print(f\"{'Team':<15} {'Comp':<7} {'LLM':<7}\")\n",
    "        print(\"-\" * 30)\n",
    "        for team_idx, team_name in enumerate(run_group_names):\n",
    "            if team_name in results['summary']:\n",
    "                comp = f\"{comp_scores[team_idx]:.2f}\"\n",
    "                llm = f\"{llm_scores[team_idx]:.2f}\"\n",
    "                print(f\"{team_name[:12]:<15} {comp:<7} {llm:<7}\")\n",
    "        \n",
    "        # Add visualization of all team scores\n",
    "        print(\"\\nTEAM SCORE COMPARISON:\")\n",
    "        max_team_score = max(\n",
    "            max(comp_scores) if comp_scores else 0,\n",
    "            max(llm_scores) if llm_scores else 0\n",
    "        )\n",
    "        team_scale = 30 / max_team_score if max_team_score > 0 else 1\n",
    "        \n",
    "        for team_idx, team_name in enumerate(run_group_names):\n",
    "            if team_name in results['summary']:\n",
    "                team_short = f\"Team {team_idx+1:02d}\"\n",
    "                comp = comp_scores[team_idx]\n",
    "                llm = llm_scores[team_idx]\n",
    "                \n",
    "                comp_bar = \"█\" * int(comp * team_scale)\n",
    "                llm_bar = \"█\" * int(llm * team_scale)\n",
    "                \n",
    "                print(f\"{team_short}: C:{comp:.1f} {comp_bar}  L:{llm:.1f} {llm_bar}\")\n",
    "        \n",
    "        # Calculate run averages\n",
    "        if comp_scores and llm_scores:\n",
    "            avg_comp_score = sum(comp_scores) / len(comp_scores)\n",
    "            avg_llm_score = sum(llm_scores) / len(llm_scores)\n",
    "            avg_llm_raw_score = sum(llm_raw_scores) / len(llm_raw_scores)\n",
    "            \n",
    "            print(\"\\nRUN AVERAGES:\")\n",
    "            print(f\"Average computational coherence: {avg_comp_score:.2f}\")\n",
    "            print(f\"Average LLM coherence (smoothed): {avg_llm_score:.2f}\")\n",
    "            print(f\"Average LLM coherence (raw): {avg_llm_raw_score:.2f}\")\n",
    "            \n",
    "            # Store run results with averages\n",
    "            all_results[run_name] = {\n",
    "                \"config\": run_config,\n",
    "                \"teams\": run_results,\n",
    "                \"averages\": {\n",
    "                    \"computational_coherence\": float(avg_comp_score),\n",
    "                    \"llm_coherence_smoothed\": float(avg_llm_score),\n",
    "                    \"llm_coherence_raw\": float(avg_llm_raw_score),\n",
    "                    \"computational_scores\": [float(s) for s in comp_scores],\n",
    "                    \"llm_scores_smoothed\": [float(s) for s in llm_scores],\n",
    "                    \"llm_scores_raw\": [float(s) for s in llm_raw_scores]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Save run summary results\n",
    "        with open(f\"{results_dir}/run_{run_idx+1}_results.json\", \"w\") as f:\n",
    "            json.dump(all_results[run_name], f, indent=2)\n",
    "        \n",
    "        # Pause between runs to avoid rate limiting\n",
    "        if run_idx < len(experiment_runs) - 1:\n",
    "            print(f\"\\nPausing for 5 seconds before next run...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # Once all runs are complete, compile the final results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL RUNS COMPLETE - FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Collect average scores for each run\n",
    "    run_names = [run[\"name\"] for run in experiment_runs]\n",
    "    avg_comp_scores = []\n",
    "    avg_llm_scores = []\n",
    "    avg_llm_raw_scores = []\n",
    "    \n",
    "    for run_name in run_names:\n",
    "        if run_name in all_results:\n",
    "            avg_comp_scores.append(all_results[run_name][\"averages\"][\"computational_coherence\"])\n",
    "            avg_llm_scores.append(all_results[run_name][\"averages\"][\"llm_coherence_smoothed\"])\n",
    "            avg_llm_raw_scores.append(all_results[run_name][\"averages\"][\"llm_coherence_raw\"])\n",
    "    \n",
    "    # Print score comparison\n",
    "    print(\"\\nAVERAGE COHERENCE SCORES BY RUN:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, run_name in enumerate(run_names):\n",
    "        if run_name in all_results:\n",
    "            mixing_pattern = experiment_runs[i][\"mixing_pattern\"]\n",
    "            comp_score = avg_comp_scores[i]\n",
    "            llm_score = avg_llm_scores[i]\n",
    "            llm_raw = avg_llm_raw_scores[i]\n",
    "            \n",
    "            print(f\"\\n{run_name}\")\n",
    "            print(f\"Mixing pattern: {mixing_pattern}\")\n",
    "            print(f\"Avg computational coherence: {comp_score:.2f}\")\n",
    "            print(f\"Avg LLM coherence (smoothed): {llm_score:.2f}\")\n",
    "            print(f\"Avg LLM coherence (raw): {llm_raw:.2f}\")\n",
    "    \n",
    "    # Check if average scores follow the expected decreasing trend\n",
    "    if len(avg_comp_scores) >= 5:\n",
    "        is_comp_decreasing = all(avg_comp_scores[i] >= avg_comp_scores[i+1] \n",
    "                                for i in range(len(avg_comp_scores)-1))\n",
    "        is_llm_decreasing = all(avg_llm_scores[i] >= avg_llm_scores[i+1] \n",
    "                               for i in range(len(avg_llm_scores)-1))\n",
    "        is_llm_raw_decreasing = all(avg_llm_raw_scores[i] >= avg_llm_raw_scores[i+1] \n",
    "                                   for i in range(len(avg_llm_raw_scores)-1))\n",
    "        \n",
    "        print(\"\\nTREND ANALYSIS:\")\n",
    "        print(f\"Computational scores follow expected decreasing trend: {is_comp_decreasing}\")\n",
    "        print(f\"LLM scores (smoothed) follow expected decreasing trend: {is_llm_decreasing}\")\n",
    "        print(f\"LLM scores (raw) follow expected decreasing trend: {is_llm_raw_decreasing}\")\n",
    "        \n",
    "        # Check if smoothing improved trend adherence\n",
    "        if is_llm_decreasing != is_llm_raw_decreasing:\n",
    "            if is_llm_decreasing:\n",
    "                print(\"Smoothing IMPROVED trend adherence (raw scores didn't follow expected trend)\")\n",
    "            else:\n",
    "                print(\"Smoothing WORSENED trend adherence (raw scores followed expected trend)\")\n",
    "        \n",
    "        # Calculate decrease percentage\n",
    "        if avg_comp_scores[0] > 0 and avg_llm_scores[0] > 0:\n",
    "            comp_decrease = ((avg_comp_scores[0] - avg_comp_scores[-1]) / avg_comp_scores[0]) * 100\n",
    "            llm_decrease = ((avg_llm_scores[0] - avg_llm_scores[-1]) / avg_llm_scores[0]) * 100\n",
    "            llm_raw_decrease = ((avg_llm_raw_scores[0] - avg_llm_raw_scores[-1]) / avg_llm_raw_scores[0]) * 100\n",
    "            \n",
    "            print(f\"Average computational coherence decreased by {comp_decrease:.1f}% from Run 1 to Run 5\")\n",
    "            print(f\"Average LLM coherence (smoothed) decreased by {llm_decrease:.1f}% from Run 1 to Run 5\")\n",
    "            print(f\"Average LLM coherence (raw) decreased by {llm_raw_decrease:.1f}% from Run 1 to Run 5\")\n",
    "    \n",
    "    # Create a simple ASCII chart \n",
    "    print(\"\\nAVERAGE COHERENCE SCORE TRENDS:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Find maximum score for scaling the chart\n",
    "    max_score = max(\n",
    "        max(avg_comp_scores) if avg_comp_scores else 0, \n",
    "        max(avg_llm_scores) if avg_llm_scores else 0,\n",
    "        max(avg_llm_raw_scores) if avg_llm_raw_scores else 0\n",
    "    )\n",
    "    scale = 40 / max_score if max_score > 0 else 1\n",
    "    \n",
    "    for i, run_name in enumerate(run_names):\n",
    "        if i < len(avg_comp_scores) and i < len(avg_llm_scores) and i < len(avg_llm_raw_scores):\n",
    "            run_short = f\"Run {i+1}\"\n",
    "            comp_score = avg_comp_scores[i]\n",
    "            llm_score = avg_llm_scores[i]\n",
    "            llm_raw = avg_llm_raw_scores[i]\n",
    "            \n",
    "            comp_bar = \"█\" * int(comp_score * scale)\n",
    "            llm_bar = \"█\" * int(llm_score * scale)\n",
    "            llm_raw_bar = \"▒\" * int(llm_raw * scale)  # Different character for raw scores\n",
    "            \n",
    "            print(f\"{run_short:8} Comp:     {comp_score:.2f} {comp_bar}\")\n",
    "            print(f\"{' ':8} LLM Raw:  {llm_raw:.2f} {llm_raw_bar}\")\n",
    "            print(f\"{' ':8} LLM Smth: {llm_score:.2f} {llm_bar}\")\n",
    "            print()\n",
    "    \n",
    "    # Save the final compiled results\n",
    "    with open(f\"{results_dir}/all_results.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"runs\": all_results,\n",
    "            \"trend_analysis\": {\n",
    "                \"avg_computational_scores\": avg_comp_scores,\n",
    "                \"avg_llm_scores_smoothed\": avg_llm_scores,\n",
    "                \"avg_llm_scores_raw\": avg_llm_raw_scores,\n",
    "                \"computational_decreasing\": is_comp_decreasing if 'is_comp_decreasing' in locals() else None,\n",
    "                \"llm_smoothed_decreasing\": is_llm_decreasing if 'is_llm_decreasing' in locals() else None,\n",
    "                \"llm_raw_decreasing\": is_llm_raw_decreasing if 'is_llm_raw_decreasing' in locals() else None,\n",
    "                \"computational_decrease_percentage\": comp_decrease if 'comp_decrease' in locals() else None,\n",
    "                \"llm_smoothed_decrease_percentage\": llm_decrease if 'llm_decrease' in locals() else None,\n",
    "                \"llm_raw_decrease_percentage\": llm_raw_decrease if 'llm_raw_decrease' in locals() else None\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Also save a summary of the experiment\n",
    "    with open(f\"{results_dir}/experiment_summary.txt\", \"w\") as f:\n",
    "        f.write(\"COHERENCE EXPERIMENTS SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Results directory: {results_dir}\\n\\n\")\n",
    "        \n",
    "        f.write(\"AVERAGE COHERENCE SCORES BY RUN:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        for i, run_name in enumerate(run_names):\n",
    "            if run_name in all_results:\n",
    "                f.write(f\"\\n{run_name}\\n\")\n",
    "                mixing_pattern = experiment_runs[i][\"mixing_pattern\"]\n",
    "                f.write(f\"Mixing pattern: {mixing_pattern}\\n\")\n",
    "                f.write(f\"Avg computational coherence: {avg_comp_scores[i]:.2f}\\n\")\n",
    "                f.write(f\"Avg LLM coherence (smoothed): {avg_llm_scores[i]:.2f}\\n\")\n",
    "                f.write(f\"Avg LLM coherence (raw): {avg_llm_raw_scores[i]:.2f}\\n\")\n",
    "        \n",
    "        if 'is_comp_decreasing' in locals():\n",
    "            f.write(\"\\nTREND ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Computational scores follow expected decreasing trend: {is_comp_decreasing}\\n\")\n",
    "            f.write(f\"LLM scores (smoothed) follow expected decreasing trend: {is_llm_decreasing}\\n\")\n",
    "            f.write(f\"LLM scores (raw) follow expected decreasing trend: {is_llm_raw_decreasing}\\n\")\n",
    "            \n",
    "            if 'comp_decrease' in locals():\n",
    "                f.write(f\"\\nAverage computational coherence decreased by {comp_decrease:.1f}% from Run 1 to Run 5\\n\")\n",
    "                f.write(f\"Average LLM coherence (smoothed) decreased by {llm_decrease:.1f}% from Run 1 to Run 5\\n\")\n",
    "                f.write(f\"Average LLM coherence (raw) decreased by {llm_raw_decrease:.1f}% from Run 1 to Run 5\\n\")\n",
    "    \n",
    "    print(f\"\\nAll run results saved to {results_dir}/ directory\")\n",
    "    print(\"\\nExperiments complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env310",
   "language": "python",
   "name": "my_env310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
